{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f03ed51",
   "metadata": {},
   "source": [
    "# 1. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d5a57b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:24:34.318265Z",
     "iopub.status.busy": "2025-05-08T05:24:34.317949Z",
     "iopub.status.idle": "2025-05-08T05:24:37.628292Z",
     "shell.execute_reply": "2025-05-08T05:24:37.627487Z",
     "shell.execute_reply.started": "2025-05-08T05:24:34.318238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from typing import Any, Optional, Tuple, Type\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import copy\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.integrate import simpson as simps\n",
    "import math\n",
    "from torchvision.models import swin_b\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "738ae701",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:24:37.629502Z",
     "iopub.status.busy": "2025-05-08T05:24:37.629187Z",
     "iopub.status.idle": "2025-05-08T05:24:37.633742Z",
     "shell.execute_reply": "2025-05-08T05:24:37.632997Z",
     "shell.execute_reply.started": "2025-05-08T05:24:37.629484Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [1] After importing libraries!!!!\n"
     ]
    }
   ],
   "source": [
    "# [LOG]\n",
    "print(\">> [1] After importing libraries!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcbd66b",
   "metadata": {},
   "source": [
    "# 2. Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aac2599",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:24:37.634645Z",
     "iopub.status.busy": "2025-05-08T05:24:37.634391Z",
     "iopub.status.idle": "2025-05-08T05:24:37.657529Z",
     "shell.execute_reply": "2025-05-08T05:24:37.656805Z",
     "shell.execute_reply.started": "2025-05-08T05:24:37.634613Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        mlp_dim: int,\n",
    "        act: Type[nn.Module] = nn.GELU,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(embedding_dim, mlp_dim)\n",
    "        self.lin2 = nn.Linear(mlp_dim, embedding_dim)\n",
    "        self.act = act()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.lin2(self.act(self.lin1(x)))\n",
    "\n",
    "\n",
    "# From https://github.com/facebookresearch/detectron2/blob/main/detectron2/layers/batch_norm.py # noqa\n",
    "# Itself from https://github.com/facebookresearch/ConvNeXt/blob/d1fa8f6fef0a165b27399986cc2bdacc92777e40/models/convnext.py#L119  # noqa\n",
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "        return x\n",
    "\n",
    "\n",
    "class TwoWayTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        depth: int,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_dim: int,\n",
    "        activation: Type[nn.Module] = nn.ReLU,\n",
    "        attention_downsample_rate: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        A transformer decoder that attends to an input image using\n",
    "        queries whose positional embedding is supplied.\n",
    "\n",
    "        Args:\n",
    "          depth (int): number of layers in the transformer\n",
    "          embedding_dim (int): the channel dimension for the input embeddings\n",
    "          num_heads (int): the number of heads for multihead attention. Must\n",
    "            divide embedding_dim\n",
    "          mlp_dim (int): the channel dimension internal to the MLP block\n",
    "          activation (nn.Module): the activation to use in the MLP block\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(depth):\n",
    "            self.layers.append(\n",
    "                TwoWayAttentionBlock(\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    mlp_dim=mlp_dim,\n",
    "                    activation=activation,\n",
    "                    attention_downsample_rate=attention_downsample_rate,\n",
    "                    skip_first_layer_pe=(i == 0),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.final_attn_token_to_image = Attention(\n",
    "            embedding_dim, num_heads, downsample_rate=attention_downsample_rate\n",
    "        )\n",
    "        self.norm_final_attn = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        image_embedding: Tensor,\n",
    "        image_pe: Tensor,\n",
    "        point_embedding: Tensor,\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          image_embedding (torch.Tensor): image to attend to. Should be shape\n",
    "            B x embedding_dim x h x w for any h and w.\n",
    "          image_pe (torch.Tensor): the positional encoding to add to the image. Must\n",
    "            have the same shape as image_embedding.\n",
    "          point_embedding (torch.Tensor): the embedding to add to the query points.\n",
    "            Must have shape B x N_points x embedding_dim for any N_points.\n",
    "\n",
    "        Returns:\n",
    "          torch.Tensor: the processed point_embedding\n",
    "          torch.Tensor: the processed image_embedding\n",
    "        \"\"\"\n",
    "        # BxCxHxW -> BxHWxC == B x N_image_tokens x C\n",
    "        bs, c, h, w = image_embedding.shape\n",
    "        image_embedding = image_embedding.flatten(2).permute(0, 2, 1)\n",
    "        image_pe = image_pe.flatten(2).permute(0, 2, 1)\n",
    "\n",
    "        # Prepare queries\n",
    "        queries = point_embedding\n",
    "        keys = image_embedding\n",
    "\n",
    "        # Apply transformer blocks and final layernorm\n",
    "        for layer in self.layers:\n",
    "            queries, keys = layer(\n",
    "                queries=queries,\n",
    "                keys=keys,\n",
    "                query_pe=point_embedding,\n",
    "                key_pe=image_pe,\n",
    "            )\n",
    "\n",
    "        # Apply the final attention layer from the points to the image\n",
    "        q = queries + point_embedding\n",
    "        k = keys + image_pe\n",
    "        attn_out = self.final_attn_token_to_image(q=q, k=k, v=keys)\n",
    "        queries = queries + attn_out\n",
    "        queries = self.norm_final_attn(queries)\n",
    "\n",
    "        return queries, keys\n",
    "\n",
    "\n",
    "class TwoWayAttentionBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_dim: int = 2048,\n",
    "        activation: Type[nn.Module] = nn.ReLU,\n",
    "        attention_downsample_rate: int = 2,\n",
    "        skip_first_layer_pe: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        A transformer block with four layers: (1) self-attention of sparse\n",
    "        inputs, (2) cross attention of sparse inputs to dense inputs, (3) mlp\n",
    "        block on sparse inputs, and (4) cross attention of dense inputs to sparse\n",
    "        inputs.\n",
    "\n",
    "        Arguments:\n",
    "          embedding_dim (int): the channel dimension of the embeddings\n",
    "          num_heads (int): the number of heads in the attention layers\n",
    "          mlp_dim (int): the hidden dimension of the mlp block\n",
    "          activation (nn.Module): the activation of the mlp block\n",
    "          skip_first_layer_pe (bool): skip the PE on the first layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attn = Attention(embedding_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.cross_attn_token_to_image = Attention(\n",
    "            embedding_dim, num_heads, downsample_rate=attention_downsample_rate\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.mlp = MLPBlock(embedding_dim, mlp_dim, activation)\n",
    "        self.norm3 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.norm4 = nn.LayerNorm(embedding_dim)\n",
    "        self.cross_attn_image_to_token = Attention(\n",
    "            embedding_dim, num_heads, downsample_rate=attention_downsample_rate\n",
    "        )\n",
    "\n",
    "        self.skip_first_layer_pe = skip_first_layer_pe\n",
    "\n",
    "    def forward(\n",
    "        self, queries: Tensor, keys: Tensor, query_pe: Tensor, key_pe: Tensor\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        # Self attention block\n",
    "        if self.skip_first_layer_pe:\n",
    "            queries = self.self_attn(q=queries, k=queries, v=queries)\n",
    "        else:\n",
    "            q = queries + query_pe\n",
    "            attn_out = self.self_attn(q=q, k=q, v=queries)\n",
    "            queries = queries + attn_out\n",
    "        queries = self.norm1(queries)\n",
    "\n",
    "        # Cross attention block, tokens attending to image embedding\n",
    "        q = queries + query_pe\n",
    "        k = keys + key_pe\n",
    "        attn_out = self.cross_attn_token_to_image(q=q, k=k, v=keys)\n",
    "        queries = queries + attn_out\n",
    "        queries = self.norm2(queries)\n",
    "\n",
    "        # MLP block\n",
    "        mlp_out = self.mlp(queries)\n",
    "        queries = queries + mlp_out\n",
    "        queries = self.norm3(queries)\n",
    "\n",
    "        # Cross attention block, image embedding attending to tokens\n",
    "        q = queries + query_pe\n",
    "        k = keys + key_pe\n",
    "        attn_out = self.cross_attn_image_to_token(q=k, k=q, v=queries)\n",
    "        keys = keys + attn_out\n",
    "        keys = self.norm4(keys)\n",
    "\n",
    "        return queries, keys\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    An attention layer that allows for downscaling the size of the embedding\n",
    "    after projection to queries, keys, and values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        downsample_rate: int = 1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.internal_dim = embedding_dim // downsample_rate\n",
    "        self.num_heads = num_heads\n",
    "        assert self.internal_dim % num_heads == 0, \"num_heads must divide embedding_dim.\"\n",
    "\n",
    "        self.q_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "        self.k_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "        self.v_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "        self.out_proj = nn.Linear(self.internal_dim, embedding_dim)\n",
    "\n",
    "    def _separate_heads(self, x: Tensor, num_heads: int) -> Tensor:\n",
    "        b, n, c = x.shape\n",
    "        x = x.reshape(b, n, num_heads, c // num_heads)\n",
    "        return x.transpose(1, 2)  # B x N_heads x N_tokens x C_per_head\n",
    "\n",
    "    def _recombine_heads(self, x: Tensor) -> Tensor:\n",
    "        b, n_heads, n_tokens, c_per_head = x.shape\n",
    "        x = x.transpose(1, 2)\n",
    "        return x.reshape(b, n_tokens, n_heads * c_per_head)  # B x N_tokens x C\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
    "        # Input projections\n",
    "        q = self.q_proj(q)\n",
    "        k = self.k_proj(k)\n",
    "        v = self.v_proj(v)\n",
    "\n",
    "        # Separate into heads\n",
    "        q = self._separate_heads(q, self.num_heads)\n",
    "        k = self._separate_heads(k, self.num_heads)\n",
    "        v = self._separate_heads(v, self.num_heads)\n",
    "\n",
    "        # Attention\n",
    "        _, _, _, c_per_head = q.shape\n",
    "        attn = q @ k.permute(0, 1, 3, 2)  # B x N_heads x N_tokens x N_tokens\n",
    "        attn = attn / math.sqrt(c_per_head)\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "\n",
    "        # Get output\n",
    "        out = attn @ v\n",
    "        out = self._recombine_heads(out)\n",
    "        out = self.out_proj(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7767f318",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:24:37.658488Z",
     "iopub.status.busy": "2025-05-08T05:24:37.658248Z",
     "iopub.status.idle": "2025-05-08T05:24:37.674472Z",
     "shell.execute_reply": "2025-05-08T05:24:37.673990Z",
     "shell.execute_reply.started": "2025-05-08T05:24:37.658471Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [2] After defining Transformer classes!!!!\n"
     ]
    }
   ],
   "source": [
    "# [LOG]\n",
    "print(\">> [2] After defining Transformer classes!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feee7e4",
   "metadata": {},
   "source": [
    "# 3. FaceXFormer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7612844d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:24:37.675857Z",
     "iopub.status.busy": "2025-05-08T05:24:37.675403Z",
     "iopub.status.idle": "2025-05-08T05:24:37.702177Z",
     "shell.execute_reply": "2025-05-08T05:24:37.701409Z",
     "shell.execute_reply.started": "2025-05-08T05:24:37.675839Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        output_dim: int,\n",
    "        num_layers: int,\n",
    "        sigmoid_output: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(\n",
    "            nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim])\n",
    "        )\n",
    "        self.sigmoid_output = sigmoid_output\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        if self.sigmoid_output:\n",
    "            x = F.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "class FaceDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        transformer_dim: 256,\n",
    "        transformer: nn.Module,\n",
    "        activation: Type[nn.Module] = nn.GELU,\n",
    "    ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.transformer_dim = transformer_dim\n",
    "        self.transformer = transformer\n",
    "\n",
    "        self.landmarks_token = nn.Embedding(1, transformer_dim)\n",
    "        self.pose_token = nn.Embedding(1, transformer_dim)\n",
    "        self.attribute_token = nn.Embedding(1, transformer_dim)\n",
    "        self.visibility_token = nn.Embedding(1, transformer_dim)\n",
    "        self.age_token = nn.Embedding(1, transformer_dim)\n",
    "        self.gender_token = nn.Embedding(1, transformer_dim)\n",
    "        self.race_token = nn.Embedding(1, transformer_dim)\n",
    "        self.mask_tokens = nn.Embedding(11, transformer_dim)\n",
    "        \n",
    "\n",
    "        self.output_upscaling = nn.Sequential(\n",
    "            nn.ConvTranspose2d(transformer_dim, transformer_dim // 4, kernel_size=2, stride=2),\n",
    "            LayerNorm2d(transformer_dim // 4),\n",
    "            activation(),\n",
    "            nn.ConvTranspose2d(transformer_dim // 4, transformer_dim // 8, kernel_size=2, stride=2),\n",
    "            activation(),\n",
    "        )\n",
    "        \n",
    "        self.output_hypernetwork_mlps = MLP(\n",
    "            transformer_dim, transformer_dim, transformer_dim // 8, 3\n",
    "            )\n",
    "                \n",
    "        self.landmarks_prediction_head = MLP(\n",
    "            transformer_dim, transformer_dim, 136, 3\n",
    "        )\n",
    "        self.pose_prediction_head = MLP(\n",
    "            transformer_dim, transformer_dim, 3, 3\n",
    "        )\n",
    "        self.attribute_prediction_head = MLP(\n",
    "            transformer_dim, transformer_dim, 40, 3\n",
    "        )\n",
    "        self.visibility_prediction_head = MLP(\n",
    "            transformer_dim, transformer_dim, 29, 3\n",
    "        )\n",
    "        self.age_prediction_head = MLP(\n",
    "            transformer_dim, transformer_dim, 8, 3\n",
    "        )\n",
    "        self.gender_prediction_head = MLP(\n",
    "            transformer_dim, transformer_dim, 2, 3\n",
    "        )\n",
    "        self.race_prediction_head = MLP(\n",
    "            transformer_dim, transformer_dim, 5, 3\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        image_embeddings: torch.Tensor,\n",
    "        image_pe: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        output_tokens = torch.cat([self.landmarks_token.weight, self.pose_token.weight, self.attribute_token.weight, self.visibility_token.weight, self.age_token.weight, self.gender_token.weight, self.race_token.weight,self.mask_tokens.weight], dim=0) \n",
    "        tokens = output_tokens.unsqueeze(0).expand(image_embeddings.size(0), -1, -1)\n",
    "\n",
    "        src = image_embeddings\n",
    "        pos_src = image_pe.expand(image_embeddings.size(0), -1, -1, -1)\n",
    "        b, c, h, w = src.shape\n",
    "\n",
    "        hs, src = self.transformer(src, pos_src, tokens)\n",
    "    \n",
    "        landmarks_token_out = hs[:, 0, :]\n",
    "        pose_token_out =  hs[:, 1, :]\n",
    "        attribute_token_out = hs[:, 2, :]\n",
    "        visibility_token_out = hs[:, 3, :]\n",
    "        age_token_out = hs[:, 4, :]\n",
    "        gender_token_out = hs[:, 5, :]\n",
    "        race_token_out = hs[:, 6, :]\n",
    "        mask_token_out =  hs[:, 7:, :]\n",
    "        \n",
    "        \n",
    "        landmark_output = self.landmarks_prediction_head(landmarks_token_out)\n",
    "        headpose_output = self.pose_prediction_head(pose_token_out)\n",
    "        attribute_output = self.attribute_prediction_head(attribute_token_out)\n",
    "        visibility_output = self.visibility_prediction_head(visibility_token_out)\n",
    "        age_output = self.age_prediction_head(age_token_out)\n",
    "        gender_output = self.gender_prediction_head(gender_token_out)\n",
    "        race_output = self.race_prediction_head(race_token_out)\n",
    "        \n",
    "        src = src.transpose(1, 2).view(b, c, h, w) \n",
    "        upscaled_embedding = self.output_upscaling(src)  \n",
    "        hyper_in = self.output_hypernetwork_mlps(mask_token_out)\n",
    "        b, c, h, w = upscaled_embedding.shape\n",
    "        seg_output = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -1, h, w)\n",
    "        \n",
    "        \n",
    "        return landmark_output, headpose_output, attribute_output, visibility_output, age_output, gender_output, race_output, seg_output\n",
    "\n",
    "\n",
    "\n",
    "class PositionEmbeddingRandom(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding using random spatial frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_pos_feats: int = 64, scale: Optional[float] = None) -> None:\n",
    "        super().__init__()\n",
    "        if scale is None or scale <= 0.0:\n",
    "            scale = 1.0\n",
    "        self.register_buffer(\n",
    "            \"positional_encoding_gaussian_matrix\",\n",
    "            scale * torch.randn((2, num_pos_feats)),\n",
    "        )\n",
    "\n",
    "    def _pe_encoding(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n",
    "        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n",
    "        coords = 2 * coords - 1\n",
    "        coords = coords @ self.positional_encoding_gaussian_matrix\n",
    "        coords = 2 * np.pi * coords\n",
    "        # outputs d_1 x ... x d_n x C shape\n",
    "        return torch.cat([torch.sin(coords), torch.cos(coords)], dim=-1)\n",
    "\n",
    "    def forward(self, size: Tuple[int, int]) -> torch.Tensor:\n",
    "        \"\"\"Generate positional encoding for a grid of the specified size.\"\"\"\n",
    "        h, w = size\n",
    "        device: Any = self.positional_encoding_gaussian_matrix.device\n",
    "        grid = torch.ones((h, w), device=device, dtype=torch.float32)\n",
    "        y_embed = grid.cumsum(dim=0) - 0.5\n",
    "        x_embed = grid.cumsum(dim=1) - 0.5\n",
    "        y_embed = y_embed / h\n",
    "        x_embed = x_embed / w\n",
    "\n",
    "        pe = self._pe_encoding(torch.stack([x_embed, y_embed], dim=-1))\n",
    "        return pe.permute(2, 0, 1)  # C x H x W\n",
    "\n",
    "    def forward_with_coords(\n",
    "        self, coords_input: torch.Tensor, image_size: Tuple[int, int]\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Positionally encode points that are not normalized to [0,1].\"\"\"\n",
    "        coords = coords_input.clone()\n",
    "        coords[:, :, 0] = coords[:, :, 0] / image_size[1]\n",
    "        coords[:, :, 1] = coords[:, :, 1] / image_size[0]\n",
    "        return self._pe_encoding(coords.to(torch.float))  # B x N x C\n",
    "\n",
    "\n",
    "class FaceXFormerMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(input_dim, 256)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        hidden_states = hidden_states.flatten(2).transpose(1, 2)\n",
    "        hidden_states = self.proj(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class FaceXFormer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FaceXFormer, self).__init__()\n",
    "\n",
    "        swin_v2 = swin_b(weights='IMAGENET1K_V1')\n",
    "        self.backbone = torch.nn.Sequential(*(list(swin_v2.children())[:-1]))\n",
    "        self.target_layer_names = ['0.1', '0.3', '0.5', '0.7']\n",
    "        self.multi_scale_features = []\n",
    "        \n",
    "\n",
    "        embed_dim = 1024\n",
    "        out_chans = 256\n",
    "        \n",
    "        self.pe_layer = PositionEmbeddingRandom(out_chans // 2)   \n",
    "\n",
    "        for name, module in self.backbone.named_modules():\n",
    "            if name in self.target_layer_names:\n",
    "                module.register_forward_hook(self.save_features_hook(name))\n",
    "        \n",
    "        self.face_decoder = FaceDecoder(\n",
    "            transformer_dim=256,\n",
    "            transformer=TwoWayTransformer(\n",
    "                depth=2,\n",
    "                embedding_dim=256,\n",
    "                mlp_dim=2048,\n",
    "                num_heads=8,\n",
    "            ))    \n",
    "        \n",
    "        num_encoder_blocks = 4\n",
    "        hidden_sizes = [128, 256, 512, 1024]\n",
    "        decoder_hidden_size = 256\n",
    "        \n",
    "        mlps = []\n",
    "        for i in range(num_encoder_blocks):\n",
    "            mlp = FaceXFormerMLP(input_dim=hidden_sizes[i])\n",
    "            mlps.append(mlp)\n",
    "        self.linear_c = nn.ModuleList(mlps)\n",
    "\n",
    "        self.linear_fuse = nn.Conv2d(\n",
    "            in_channels=decoder_hidden_size * num_encoder_blocks,\n",
    "            out_channels=decoder_hidden_size,\n",
    "            kernel_size=1,\n",
    "            bias=False,\n",
    "        )\n",
    "    \n",
    "    def save_features_hook(self, name):\n",
    "        def hook(module, input, output):\n",
    "            self.multi_scale_features.append(output.permute(0,3,1,2).contiguous()) \n",
    "        return hook\n",
    "\n",
    "    def forward(self, x, labels, tasks):\n",
    "        self.multi_scale_features.clear()\n",
    "        \n",
    "        _,_,h,w = x.shape\n",
    "        features = self.backbone(x).squeeze()\n",
    "        \n",
    "        \n",
    "        batch_size = self.multi_scale_features[-1].shape[0]\n",
    "        all_hidden_states = ()\n",
    "        for encoder_hidden_state, mlp in zip(self.multi_scale_features, self.linear_c):\n",
    "        \n",
    "            height, width = encoder_hidden_state.shape[2], encoder_hidden_state.shape[3]\n",
    "            encoder_hidden_state = mlp(encoder_hidden_state)\n",
    "            encoder_hidden_state = encoder_hidden_state.permute(0, 2, 1)\n",
    "            encoder_hidden_state = encoder_hidden_state.reshape(batch_size, -1, height, width)\n",
    "            encoder_hidden_state = nn.functional.interpolate(\n",
    "                encoder_hidden_state, size=self.multi_scale_features[0].size()[2:], mode=\"bilinear\", align_corners=False\n",
    "            )\n",
    "            all_hidden_states += (encoder_hidden_state,)\n",
    "        \n",
    "        fused_states = self.linear_fuse(torch.cat(all_hidden_states[::-1], dim=1))\n",
    "        image_pe = self.pe_layer((fused_states.shape[2], fused_states.shape[3])).unsqueeze(0)\n",
    "        \n",
    "        landmark_output, headpose_output, attribute_output, visibility_output, age_output, gender_output, race_output, seg_output = self.face_decoder(\n",
    "                image_embeddings=fused_states,\n",
    "                image_pe=image_pe\n",
    "            )\n",
    "        \n",
    "        segmentation_indices = (tasks == 0)\n",
    "        seg_output = seg_output[segmentation_indices]\n",
    "        \n",
    "        landmarks_indices = (tasks == 1)\n",
    "        landmark_output = landmark_output[landmarks_indices]\n",
    "\n",
    "        headpose_indices = (tasks == 2)\n",
    "        headpose_output = headpose_output[headpose_indices]\n",
    "        \n",
    "        attribute_indices = (tasks == 3)\n",
    "        attribute_output = attribute_output[attribute_indices]\n",
    "\n",
    "        age_indices = (tasks == 4)\n",
    "        age_output = age_output[age_indices]\n",
    "        gender_output = gender_output[age_indices]\n",
    "        race_output = race_output[age_indices]\n",
    "        \n",
    "        visibility_indices = (tasks == 5)\n",
    "        visibility_output = visibility_output[visibility_indices]\n",
    "    \n",
    "        # return landmark_output, headpose_output, attribute_output, visibility_output, age_output, gender_output, race_output, seg_output\n",
    "        return landmark_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2386999",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:24:37.704976Z",
     "iopub.status.busy": "2025-05-08T05:24:37.704495Z",
     "iopub.status.idle": "2025-05-08T05:24:37.717266Z",
     "shell.execute_reply": "2025-05-08T05:24:37.716584Z",
     "shell.execute_reply.started": "2025-05-08T05:24:37.704958Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [3] After defining FaceXFormer model!!!!\n"
     ]
    }
   ],
   "source": [
    "# [LOG]\n",
    "print(\">> [3] After defining FaceXFormer model!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44e2baf",
   "metadata": {},
   "source": [
    "# 4. Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca4ec1dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:24:37.718179Z",
     "iopub.status.busy": "2025-05-08T05:24:37.717962Z",
     "iopub.status.idle": "2025-05-08T05:24:37.730072Z",
     "shell.execute_reply": "2025-05-08T05:24:37.729482Z",
     "shell.execute_reply.started": "2025-05-08T05:24:37.718164Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "class Accuracy:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Accuracy()\"\n",
    "\n",
    "    def test(self, label_pd, label_gt, ignore_label=-1):\n",
    "        correct_cnt = 0\n",
    "        total_cnt = 0\n",
    "        with torch.no_grad():\n",
    "            label_pd = F.softmax(label_pd, dim=1)\n",
    "            label_pd = torch.max(label_pd, 1)[1]\n",
    "            label_gt = label_gt.long()\n",
    "            c = (label_pd == label_gt)\n",
    "            correct_cnt = torch.sum(c).item()\n",
    "            total_cnt = c.size(0) - torch.sum(label_gt == ignore_label).item()\n",
    "        return correct_cnt, total_cnt\n",
    "\n",
    "\n",
    "# FR_AUC\n",
    "class FR_AUC:\n",
    "\n",
    "    def __init__(self, data_definition):\n",
    "        self.data_definition = data_definition\n",
    "        if data_definition == '300W':\n",
    "            self.thresh = 0.05\n",
    "        else:\n",
    "            self.thresh = 0.1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"FR_AUC()\"\n",
    "\n",
    "    def test(self, nmes, thres=None, step=0.0001):\n",
    "        if thres is None:\n",
    "            thres = self.thresh\n",
    "\n",
    "        num_data = len(nmes)\n",
    "        xs = np.arange(0, thres + step, step)\n",
    "        ys = np.array([np.count_nonzero(nmes <= x)\n",
    "                      for x in xs]) / float(num_data)\n",
    "        fr = 1.0 - ys[-1]\n",
    "        auc = simps(ys, x=xs) / thres\n",
    "        return [round(fr, 4), round(auc, 6)]\n",
    "\n",
    "\n",
    "# Normalized Mean Error\n",
    "class NME:\n",
    "\n",
    "    def __init__(self, nme_left_index, nme_right_index):\n",
    "        self.nme_left_index = nme_left_index\n",
    "        self.nme_right_index = nme_right_index\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"NME()\"\n",
    "\n",
    "    def get_norm_distance(self, landmarks):\n",
    "        assert isinstance(self.nme_right_index,\n",
    "                          list), 'the nme_right_index is not list.'\n",
    "        assert isinstance(self.nme_left_index,\n",
    "                          list), 'the nme_left, index is not list.'\n",
    "        right_pupil = landmarks[self.nme_right_index, :].mean(0)\n",
    "        left_pupil = landmarks[self.nme_left_index, :].mean(0)\n",
    "        norm_distance = np.linalg.norm(right_pupil - left_pupil)\n",
    "        return norm_distance\n",
    "\n",
    "    def test(self, label_pd, label_gt):\n",
    "        nme_list = []\n",
    "        label_pd = label_pd.data.cpu().numpy()\n",
    "        label_gt = label_gt.data.cpu().numpy()\n",
    "\n",
    "        for i in range(label_gt.shape[0]):\n",
    "            landmarks_gt = label_gt[i]\n",
    "            landmarks_pv = label_pd[i]\n",
    "            if isinstance(self.nme_right_index, list):\n",
    "                norm_distance = self.get_norm_distance(landmarks_gt)\n",
    "            elif isinstance(self.nme_right_index, int):\n",
    "                norm_distance = np.linalg.norm(\n",
    "                    landmarks_gt[self.nme_left_index] - landmarks_gt[self.nme_right_index])\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            landmarks_delta = landmarks_pv - landmarks_gt\n",
    "            nme = (np.linalg.norm(landmarks_delta, axis=1) / norm_distance).mean()\n",
    "            nme_list.append(nme)\n",
    "            # sum_nme += nme\n",
    "            # total_cnt += 1\n",
    "        return nme_list\n",
    "\n",
    "\n",
    "# Count parameters in MB\n",
    "def count_parameters_in_MB(model):\n",
    "    if isinstance(model, nn.Module):\n",
    "        return sum(v.numel() for v in model.parameters()) / 1e6\n",
    "    else:\n",
    "        return sum(v.numel() for v in model) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faddfd29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:24:37.730962Z",
     "iopub.status.busy": "2025-05-08T05:24:37.730713Z",
     "iopub.status.idle": "2025-05-08T05:24:37.744376Z",
     "shell.execute_reply": "2025-05-08T05:24:37.743813Z",
     "shell.execute_reply.started": "2025-05-08T05:24:37.730939Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [4] After defining metrics!!!!\n"
     ]
    }
   ],
   "source": [
    "# [LOG]\n",
    "print(\">> [4] After defining metrics!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba0f845",
   "metadata": {},
   "source": [
    "# 5. Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8849e151",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:24:37.745234Z",
     "iopub.status.busy": "2025-05-08T05:24:37.745005Z",
     "iopub.status.idle": "2025-05-08T05:24:37.755142Z",
     "shell.execute_reply": "2025-05-08T05:24:37.754567Z",
     "shell.execute_reply.started": "2025-05-08T05:24:37.745214Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Face300WXMLDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset cho iBUG 300-W với XML annotations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, xml_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        xml_path = os.path.join(root_dir, xml_file)\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        for img in root.findall('images/image'):\n",
    "            rel_path = img.get('file')\n",
    "            img_path = os.path.join(root_dir, rel_path)\n",
    "            parts = img.find('box').findall('part')\n",
    "            lm = np.zeros((len(parts), 2), dtype=np.float16)\n",
    "            for p in parts:\n",
    "                idx = int(p.get('name'))\n",
    "                lm[idx, 0] = np.float16(p.get('x'))\n",
    "                lm[idx, 1] = np.float16(p.get('y'))\n",
    "            self.samples.append({\"img_path\": img_path, \"landmarks\": lm})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        image = Image.open(sample['img_path']).convert('RGB')\n",
    "        landmarks = torch.from_numpy(sample['landmarks'])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return {\"image\": image, \"landmarks\": landmarks}\n",
    "\n",
    "    # Apply the transform to all samples\n",
    "    def apply_transform(self, transform) -> None:\n",
    "        for sample in self.samples:\n",
    "            image = Image.open(sample['img_path']).convert('RGB')\n",
    "            sample['image'] = transform(image)\n",
    "            sample['landmarks'] = torch.from_numpy(sample['landmarks']).float()\n",
    "\n",
    "    # Set the transform for the dataset\n",
    "    def set_transform(self, transform):\n",
    "        self.transform = transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0b74375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:24:37.756313Z",
     "iopub.status.busy": "2025-05-08T05:24:37.756072Z",
     "iopub.status.idle": "2025-05-08T05:24:38.144986Z",
     "shell.execute_reply": "2025-05-08T05:24:38.144437Z",
     "shell.execute_reply.started": "2025-05-08T05:24:37.756292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Đường dẫn dataset\n",
    "if os.path.exists('./ibug_300W_large_face_landmark_dataset'):\n",
    "    root = './ibug_300W_large_face_landmark_dataset'\n",
    "else:\n",
    "    root = '/kaggle/input/ibug-300w-large-face-landmark-dataset/ibug_300W_large_face_landmark_dataset'\n",
    "test_xml = 'labels_ibug_300W_test.xml'\n",
    "\n",
    "# Định nghĩa transform cho test\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# Dataset\n",
    "test_ds = Face300WXMLDataset(root, test_xml, transform=val_test_transform)\n",
    "\n",
    "# Set transform cho train/val/test\n",
    "test_ds.set_transform(val_test_transform)\n",
    "\n",
    "# DataLoader với batch_size=48\n",
    "test_loader = DataLoader(test_ds, batch_size=48, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01b4d18b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:24:38.145913Z",
     "iopub.status.busy": "2025-05-08T05:24:38.145658Z",
     "iopub.status.idle": "2025-05-08T05:24:38.149960Z",
     "shell.execute_reply": "2025-05-08T05:24:38.149410Z",
     "shell.execute_reply.started": "2025-05-08T05:24:38.145897Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [5] After defining dataset and dataloader!!!!\n"
     ]
    }
   ],
   "source": [
    "# [LOG]\n",
    "print(\">> [5] After defining dataset and dataloader!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf3af2b",
   "metadata": {},
   "source": [
    "# 6. Test Pretrained Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289b7caf",
   "metadata": {},
   "source": [
    "## 6.1. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb4bd0d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:24:38.150909Z",
     "iopub.status.busy": "2025-05-08T05:24:38.150683Z",
     "iopub.status.idle": "2025-05-08T05:24:38.214305Z",
     "shell.execute_reply": "2025-05-08T05:24:38.213703Z",
     "shell.execute_reply.started": "2025-05-08T05:24:38.150883Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31ceba29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:24:38.215278Z",
     "iopub.status.busy": "2025-05-08T05:24:38.215046Z",
     "iopub.status.idle": "2025-05-08T05:24:38.225427Z",
     "shell.execute_reply": "2025-05-08T05:24:38.224888Z",
     "shell.execute_reply.started": "2025-05-08T05:24:38.215263Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Print the used device\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e59f160",
   "metadata": {},
   "source": [
    "## 6.2. Model and Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b84adf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:24:38.226272Z",
     "iopub.status.busy": "2025-05-08T05:24:38.226093Z",
     "iopub.status.idle": "2025-05-08T05:24:42.033167Z",
     "shell.execute_reply": "2025-05-08T05:24:42.032593Z",
     "shell.execute_reply.started": "2025-05-08T05:24:38.226259Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_84/466180601.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(weights_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = FaceXFormer().to(device)\n",
    "if os.path.exists(\"ckpts/facexformer_pretrained.pth\"):\n",
    "    weights_path = \"ckpts/facexformer_pretrained.pth\"\n",
    "    checkpoint = torch.load(weights_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "elif os.path.exists(\"/kaggle/input/facexformer/pytorch/default/1/model.pt\"):\n",
    "    weights_path = \"/kaggle/input/facexformer/pytorch/default/1/model.pt\"\n",
    "    checkpoint = torch.load(weights_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['state_dict_backbone'])\n",
    "else:\n",
    "    raise FileNotFoundError(\"Model weights not found. Please check the path.\")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Metrics\n",
    "nme_metric = NME(nme_left_index=[36, 39],\n",
    "                 nme_right_index=[42, 45])  # eye landmarks for normalization\n",
    "# uses simps for AUC :contentReference[oaicite:3]{index=3}\n",
    "fr_auc_metric = FR_AUC(data_definition='300W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf9ce410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:24:42.034020Z",
     "iopub.status.busy": "2025-05-08T05:24:42.033814Z",
     "iopub.status.idle": "2025-05-08T05:24:42.040233Z",
     "shell.execute_reply": "2025-05-08T05:24:42.039426Z",
     "shell.execute_reply.started": "2025-05-08T05:24:42.034004Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA enabled with automatic mixed precision\n",
      "GPU memory allocated: 1.49 GB\n",
      "Model size: 91.99 MB\n"
     ]
    }
   ],
   "source": [
    "# Enable CUDA optimization for half precision (if available)\n",
    "if device.type == 'cuda':\n",
    "    # Enable autocast for mixed precision training\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    print(\"CUDA enabled with automatic mixed precision\")\n",
    "else:\n",
    "    scaler = None\n",
    "    print(\"Running on CPU, mixed precision not enabled\")\n",
    "\n",
    "# Report memory usage\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    print(f\"Model size: {count_parameters_in_MB(model):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a9a7f4",
   "metadata": {},
   "source": [
    "## 6.3. Evaluation on Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c72caae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:24:42.041345Z",
     "iopub.status.busy": "2025-05-08T05:24:42.041125Z",
     "iopub.status.idle": "2025-05-08T05:24:42.055072Z",
     "shell.execute_reply": "2025-05-08T05:24:42.054389Z",
     "shell.execute_reply.started": "2025-05-08T05:24:42.041330Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def denorm_points(points, h, w, align_corners=False):\n",
    "    if align_corners:\n",
    "        denorm_points = (points + 1) / 2 * torch.tensor([w - 1, h - 1], dtype=torch.float32).to(points).view(1, 1, 2)\n",
    "    else:\n",
    "        denorm_points = ((points + 1) * torch.tensor([w, h], dtype=torch.float32).to(points).view(1, 1, 2) - 1) / 2\n",
    "\n",
    "    return denorm_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec6dde92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:24:42.055960Z",
     "iopub.status.busy": "2025-05-08T05:24:42.055775Z",
     "iopub.status.idle": "2025-05-08T05:25:22.901929Z",
     "shell.execute_reply": "2025-05-08T05:25:22.901170Z",
     "shell.execute_reply.started": "2025-05-08T05:24:42.055946Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Testing]: 100%|██████████| 21/21 [00:40<00:00,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "  Mean NME    : 4.3664\n",
      "  FR (@0.05)  : 1.0000\n",
      "  AUC         : 0.000000\n",
      "  Inference/s : 137.7 img/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Evaluate on test\n",
    "test_nmes = []\n",
    "inference_times = []\n",
    "with torch.no_grad():\n",
    "    test_pbar = tqdm(test_loader, desc=\"[Testing]\")\n",
    "    for batch in test_pbar:\n",
    "        imgs = batch['image'].to(device)\n",
    "        targets = batch['landmarks'].to(device, dtype=torch.float32)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, dtype=torch.float16):\n",
    "            start = time.time()\n",
    "            preds = model(\n",
    "                imgs,\n",
    "                labels=None,\n",
    "                tasks=torch.tensor(\n",
    "                    [1] * batch_size, dtype=torch.int8).to(device)  # landmarks\n",
    "            )\n",
    "            end = time.time()\n",
    "\n",
    "            denorm_lms = denorm_points(preds.view(-1, 68, 2), 224, 224)\n",
    "            test_nmes.extend(nme_metric.test(denorm_lms, targets))\n",
    "\n",
    "        inference_times.append((end - start) / imgs.size(0))\n",
    "\n",
    "# Metrics\n",
    "mean_test_nme = sum(test_nmes) / len(test_nmes)\n",
    "# [FR, AUC] using Simpson's rule :contentReference[oaicite:9]{index=9}\n",
    "fr, auc = fr_auc_metric.test(test_nmes)\n",
    "mean_inference_time = sum(inference_times) / len(inference_times)\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"  Mean NME    : {mean_test_nme:.4f}\")\n",
    "print(f\"  FR (@0.05)  : {fr:.4f}\")\n",
    "print(f\"  AUC         : {auc:.6f}\")\n",
    "print(f\"  Inference/s : {1/mean_inference_time:.1f} img/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe74aec5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T05:25:22.902908Z",
     "iopub.status.busy": "2025-05-08T05:25:22.902691Z",
     "iopub.status.idle": "2025-05-08T05:25:22.907082Z",
     "shell.execute_reply": "2025-05-08T05:25:22.906285Z",
     "shell.execute_reply.started": "2025-05-08T05:25:22.902885Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [6] After evaluating the model on test set!!!!\n"
     ]
    }
   ],
   "source": [
    "# [LOG]\n",
    "print(\">> [6] After evaluating the model on test set!!!!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2029045,
     "sourceId": 3364057,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 285367,
     "modelInstanceId": 264270,
     "sourceId": 311589,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
