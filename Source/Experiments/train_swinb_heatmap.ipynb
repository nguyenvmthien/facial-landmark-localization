{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b9b075",
   "metadata": {
    "papermill": {
     "duration": 0.009784,
     "end_time": "2025-05-04T18:26:17.609142",
     "exception": false,
     "start_time": "2025-05-04T18:26:17.599358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca43f77b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:17.627744Z",
     "iopub.status.busy": "2025-05-04T18:26:17.627474Z",
     "iopub.status.idle": "2025-05-04T18:26:36.428665Z",
     "shell.execute_reply": "2025-05-04T18:26:36.428055Z"
    },
    "papermill": {
     "duration": 18.812242,
     "end_time": "2025-05-04T18:26:36.430091",
     "exception": false,
     "start_time": "2025-05-04T18:26:17.617849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from typing import Any, Optional, Tuple, Type\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import copy\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.integrate import simpson as simps\n",
    "import math\n",
    "from torchvision.models import swin_b\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "867cf300",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:36.448123Z",
     "iopub.status.busy": "2025-05-04T18:26:36.447771Z",
     "iopub.status.idle": "2025-05-04T18:26:36.452299Z",
     "shell.execute_reply": "2025-05-04T18:26:36.451459Z"
    },
    "papermill": {
     "duration": 0.014532,
     "end_time": "2025-05-04T18:26:36.453527",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.438995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [1] After importing libraries!!!!\n"
     ]
    }
   ],
   "source": [
    "# [LOG]\n",
    "print(\">> [1] After importing libraries!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02b8141",
   "metadata": {
    "papermill": {
     "duration": 0.007871,
     "end_time": "2025-05-04T18:26:36.469925",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.462054",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aaa9f79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:36.487514Z",
     "iopub.status.busy": "2025-05-04T18:26:36.487245Z",
     "iopub.status.idle": "2025-05-04T18:26:36.492045Z",
     "shell.execute_reply": "2025-05-04T18:26:36.491511Z"
    },
    "papermill": {
     "duration": 0.014618,
     "end_time": "2025-05-04T18:26:36.493131",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.478513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    \"\"\" MLP Block for Transformer \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embedding_dim: int,\n",
    "                 mlp_dim: int,\n",
    "                 activation: Type[nn.Module] = nn.GELU) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(embedding_dim, mlp_dim)\n",
    "        self.linear_2 = nn.Linear(mlp_dim, embedding_dim)\n",
    "        self.activation = activation()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.linear_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3d40816",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:36.510291Z",
     "iopub.status.busy": "2025-05-04T18:26:36.510065Z",
     "iopub.status.idle": "2025-05-04T18:26:36.514973Z",
     "shell.execute_reply": "2025-05-04T18:26:36.514505Z"
    },
    "papermill": {
     "duration": 0.014754,
     "end_time": "2025-05-04T18:26:36.516009",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.501255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From https://github.com/facebookresearch/detectron2/blob/main/detectron2/layers/batch_norm.py # noqa\n",
    "# Itself from https://github.com/facebookresearch/ConvNeXt/blob/d1fa8f6fef0a165b27399986cc2bdacc92777e40/models/convnext.py#L119  # noqa\n",
    "class LayerNorm2d(nn.Module):\n",
    "\n",
    "    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b53fe89b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:36.533551Z",
     "iopub.status.busy": "2025-05-04T18:26:36.533265Z",
     "iopub.status.idle": "2025-05-04T18:26:36.541252Z",
     "shell.execute_reply": "2025-05-04T18:26:36.540713Z"
    },
    "papermill": {
     "duration": 0.017969,
     "end_time": "2025-05-04T18:26:36.542394",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.524425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    An attention layer that allows for downscaling the size of the embedding\n",
    "    after projection to queries, keys, and values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        downsample_rate: int = 1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.internal_dim = embedding_dim // downsample_rate\n",
    "        self.num_heads = num_heads\n",
    "        assert self.internal_dim % num_heads == 0, \"num_heads must divide embedding_dim.\"\n",
    "\n",
    "        self.q_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "        self.k_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "        self.v_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "        self.out_proj = nn.Linear(self.internal_dim, embedding_dim)\n",
    "\n",
    "    def _separate_heads(self, x: Tensor, num_heads: int) -> Tensor:\n",
    "        b, n, c = x.shape\n",
    "        x = x.reshape(b, n, num_heads, c // num_heads)\n",
    "        return x.transpose(1, 2)  # B x N_heads x N_tokens x C_per_head\n",
    "\n",
    "    def _recombine_heads(self, x: Tensor) -> Tensor:\n",
    "        b, n_heads, n_tokens, c_per_head = x.shape\n",
    "        x = x.transpose(1, 2)\n",
    "        return x.reshape(b, n_tokens, n_heads * c_per_head)  # B x N_tokens x C\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
    "        # Input projections\n",
    "        q = self.q_proj(q)\n",
    "        k = self.k_proj(k)\n",
    "        v = self.v_proj(v)\n",
    "\n",
    "        # Separate into heads\n",
    "        q = self._separate_heads(q, self.num_heads)\n",
    "        k = self._separate_heads(k, self.num_heads)\n",
    "        v = self._separate_heads(v, self.num_heads)\n",
    "\n",
    "        # Attention\n",
    "        _, _, _, c_per_head = q.shape\n",
    "        attn = q @ k.permute(0, 1, 3, 2)  # B x N_heads x N_tokens x N_tokens\n",
    "        attn = attn / math.sqrt(c_per_head)\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "\n",
    "        # Get output\n",
    "        out = attn @ v\n",
    "        out = self._recombine_heads(out)\n",
    "        out = self.out_proj(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "942a6397",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:36.559749Z",
     "iopub.status.busy": "2025-05-04T18:26:36.559566Z",
     "iopub.status.idle": "2025-05-04T18:26:36.566930Z",
     "shell.execute_reply": "2025-05-04T18:26:36.566397Z"
    },
    "papermill": {
     "duration": 0.017301,
     "end_time": "2025-05-04T18:26:36.567900",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.550599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TwoWayAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_dim: int = 2048,\n",
    "        activation: Type[nn.Module] = nn.ReLU,\n",
    "        attention_downsample_rate: int = 2,\n",
    "        skip_first_layer_pe: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        A transformer block with four layers: (1) self-attention of sparse\n",
    "        inputs, (2) cross attention of sparse inputs to dense inputs, (3) mlp\n",
    "        block on sparse inputs, and (4) cross attention of dense inputs to sparse\n",
    "        inputs.\n",
    "\n",
    "        Arguments:\n",
    "          embedding_dim (int): the channel dimension of the embeddings\n",
    "          num_heads (int): the number of heads in the attention layers\n",
    "          mlp_dim (int): the hidden dimension of the mlp block\n",
    "          activation (nn.Module): the activation of the mlp block\n",
    "          skip_first_layer_pe (bool): skip the PE on the first layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attn = Attention(embedding_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.cross_attn_token_to_image = Attention(\n",
    "            embedding_dim, num_heads,\n",
    "            downsample_rate=attention_downsample_rate\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.mlp = MLPBlock(embedding_dim, mlp_dim, activation)\n",
    "        self.norm3 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.cross_attn_image_to_token = Attention(\n",
    "            embedding_dim, num_heads,\n",
    "            downsample_rate=attention_downsample_rate\n",
    "        )\n",
    "        self.norm4 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.skip_first_layer_pe = skip_first_layer_pe\n",
    "\n",
    "    def forward(\n",
    "        self, queries: Tensor, keys: Tensor, query_pe: Tensor, key_pe: Tensor\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        # Self attention block\n",
    "        if self.skip_first_layer_pe:\n",
    "            queries = self.self_attn(q=queries, k=queries, v=queries)\n",
    "        else:\n",
    "            q = queries + query_pe\n",
    "            attn_out = self.self_attn(q=q, k=q, v=queries)\n",
    "            queries = queries + attn_out\n",
    "        queries = self.norm1(queries)\n",
    "\n",
    "        # Cross attention block, tokens attending to image embedding\n",
    "        q = queries + query_pe\n",
    "        k = keys + key_pe\n",
    "        attn_out = self.cross_attn_token_to_image(q=q, k=k, v=keys)\n",
    "        queries = queries + attn_out\n",
    "        queries = self.norm2(queries)\n",
    "\n",
    "        # MLP block\n",
    "        mlp_out = self.mlp(queries)\n",
    "        queries = queries + mlp_out\n",
    "        queries = self.norm3(queries)\n",
    "\n",
    "        # Cross attention block, image embedding attending to tokens\n",
    "        q = queries + query_pe\n",
    "        k = keys + key_pe\n",
    "        attn_out = self.cross_attn_image_to_token(q=k, k=q, v=queries)\n",
    "        keys = keys + attn_out\n",
    "        keys = self.norm4(keys)\n",
    "\n",
    "        return queries, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48281aa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:36.585060Z",
     "iopub.status.busy": "2025-05-04T18:26:36.584870Z",
     "iopub.status.idle": "2025-05-04T18:26:36.591842Z",
     "shell.execute_reply": "2025-05-04T18:26:36.591334Z"
    },
    "papermill": {
     "duration": 0.016812,
     "end_time": "2025-05-04T18:26:36.592888",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.576076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TwoWayTransformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        depth: int,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_dim: int,\n",
    "        activation: Type[nn.Module] = nn.ReLU,\n",
    "        attention_downsample_rate: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        A transformer decoder that attends to an input image using\n",
    "        queries whose positional embedding is supplied.\n",
    "\n",
    "        Args:\n",
    "          depth (int): number of layers in the transformer\n",
    "          embedding_dim (int): the channel dimension for the input embeddings\n",
    "          num_heads (int): the number of heads for multihead attention. Must\n",
    "            divide embedding_dim\n",
    "          mlp_dim (int): the channel dimension internal to the MLP block\n",
    "          activation (nn.Module): the activation to use in the MLP block\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(depth):\n",
    "            self.layers.append(\n",
    "                TwoWayAttentionBlock(\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    mlp_dim=mlp_dim,\n",
    "                    activation=activation,\n",
    "                    attention_downsample_rate=attention_downsample_rate,\n",
    "                    skip_first_layer_pe=(i == 0),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.final_attn_token_to_image = Attention(\n",
    "            embedding_dim, num_heads,\n",
    "            downsample_rate=attention_downsample_rate\n",
    "        )\n",
    "        self.norm_final_attn = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        image_embedding: Tensor,\n",
    "        image_pe: Tensor,\n",
    "        point_embedding: Tensor,\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          image_embedding (torch.Tensor): image to attend to. Should be shape\n",
    "            B x embedding_dim x h x w for any h and w.\n",
    "          image_pe (torch.Tensor): the positional encoding to add to the image. Must\n",
    "            have the same shape as image_embedding.\n",
    "          point_embedding (torch.Tensor): the embedding to add to the query points.\n",
    "            Must have shape B x N_points x embedding_dim for any N_points.\n",
    "\n",
    "        Returns:\n",
    "          torch.Tensor: the processed point_embedding\n",
    "          torch.Tensor: the processed image_embedding\n",
    "        \"\"\"\n",
    "        # BxCxHxW -> BxHWxC == B x N_image_tokens x C\n",
    "        bs, c, h, w = image_embedding.shape\n",
    "        image_embedding = image_embedding.flatten(2).permute(0, 2, 1)\n",
    "        image_pe = image_pe.flatten(2).permute(0, 2, 1)\n",
    "\n",
    "        # Prepare queries\n",
    "        queries = point_embedding\n",
    "        keys = image_embedding\n",
    "\n",
    "        # Apply transformer blocks and final layernorm\n",
    "        for layer in self.layers:\n",
    "            queries, keys = layer(\n",
    "                queries=queries,\n",
    "                keys=keys,\n",
    "                query_pe=point_embedding,\n",
    "                key_pe=image_pe,\n",
    "            )\n",
    "\n",
    "        # Apply the final attention layer from the points to the image\n",
    "        q = queries + point_embedding\n",
    "        k = keys + image_pe\n",
    "        attn_out = self.final_attn_token_to_image(q=q, k=k, v=keys)\n",
    "        queries = queries + attn_out\n",
    "        queries = self.norm_final_attn(queries)\n",
    "\n",
    "        return queries, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78b5334b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:36.610653Z",
     "iopub.status.busy": "2025-05-04T18:26:36.610464Z",
     "iopub.status.idle": "2025-05-04T18:26:36.614311Z",
     "shell.execute_reply": "2025-05-04T18:26:36.613613Z"
    },
    "papermill": {
     "duration": 0.0141,
     "end_time": "2025-05-04T18:26:36.615294",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.601194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [2] After defining Transformer classes!!!!\n"
     ]
    }
   ],
   "source": [
    "# [LOG]\n",
    "print(\">> [2] After defining Transformer classes!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4416bca",
   "metadata": {
    "papermill": {
     "duration": 0.008006,
     "end_time": "2025-05-04T18:26:36.631396",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.623390",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. FaceXFormer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23af42e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:36.648533Z",
     "iopub.status.busy": "2025-05-04T18:26:36.648341Z",
     "iopub.status.idle": "2025-05-04T18:26:36.653682Z",
     "shell.execute_reply": "2025-05-04T18:26:36.652920Z"
    },
    "papermill": {
     "duration": 0.01533,
     "end_time": "2025-05-04T18:26:36.654790",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.639460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        output_dim: int,\n",
    "        num_layers: int,\n",
    "        sigmoid_output: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(\n",
    "            nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim])\n",
    "        )\n",
    "        self.sigmoid_output = sigmoid_output\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        if self.sigmoid_output:\n",
    "            x = F.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43791d44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:36.672579Z",
     "iopub.status.busy": "2025-05-04T18:26:36.672386Z",
     "iopub.status.idle": "2025-05-04T18:26:36.679521Z",
     "shell.execute_reply": "2025-05-04T18:26:36.678793Z"
    },
    "papermill": {
     "duration": 0.017408,
     "end_time": "2025-05-04T18:26:36.680665",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.663257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FaceDecoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        transformer_dim: int = 256,\n",
    "        transformer: nn.Module,\n",
    "        num_landmarks: int = 68,\n",
    "        activation: Type[nn.Module] = nn.GELU,\n",
    "        heatmap_size: int = 64,  # Size of the output heatmap\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.transformer_dim = transformer_dim\n",
    "        self.transformer = transformer\n",
    "        self.num_landmarks = num_landmarks\n",
    "        self.heatmap_size = heatmap_size\n",
    "\n",
    "        # Use 68 landmark tokens \n",
    "        self.landmarks_token = nn.Embedding(num_landmarks, transformer_dim)\n",
    "        \n",
    "        # Feature extraction for each landmark\n",
    "        self.landmark_feature_head = MLP(\n",
    "            transformer_dim, transformer_dim, transformer_dim, 2\n",
    "        )\n",
    "        \n",
    "        # Heatmap generation network\n",
    "        self.heatmap_generator = nn.Sequential(\n",
    "            nn.Linear(transformer_dim, 512),\n",
    "            activation(),\n",
    "            nn.Linear(512, self.heatmap_size * self.heatmap_size)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        image_embeddings: Tensor,\n",
    "        image_pe: Tensor,\n",
    "    ) -> Tensor:\n",
    "        # Get all landmark tokens\n",
    "        tokens = self.landmarks_token.weight.unsqueeze(0).expand(\n",
    "            image_embeddings.size(0), -1, -1)  # [batch_size, num_landmarks, dim]\n",
    "\n",
    "        src = image_embeddings\n",
    "        pos_src = image_pe.expand(image_embeddings.size(0), -1, -1, -1)\n",
    "        \n",
    "        # Process through transformer\n",
    "        hs, _ = self.transformer(src, pos_src, tokens)\n",
    "        \n",
    "        # Extract features for each landmark\n",
    "        landmark_features = self.landmark_feature_head(hs)  # [batch_size, num_landmarks, dim]\n",
    "        \n",
    "        # Generate heatmap for each landmark\n",
    "        batch_size = image_embeddings.size(0)\n",
    "        heatmaps_flat = self.heatmap_generator(landmark_features)\n",
    "        \n",
    "        # Reshape to proper heatmap dimensions\n",
    "        heatmaps = heatmaps_flat.view(batch_size, self.num_landmarks, self.heatmap_size, self.heatmap_size)\n",
    "        \n",
    "        # Apply softmax to create proper probability distributions\n",
    "        heatmaps = F.softmax(heatmaps.view(batch_size, self.num_landmarks, -1), dim=2).view(\n",
    "            batch_size, self.num_landmarks, self.heatmap_size, self.heatmap_size\n",
    "        )\n",
    "        \n",
    "        return heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6521bc08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:36.697490Z",
     "iopub.status.busy": "2025-05-04T18:26:36.697260Z",
     "iopub.status.idle": "2025-05-04T18:26:36.704579Z",
     "shell.execute_reply": "2025-05-04T18:26:36.703827Z"
    },
    "papermill": {
     "duration": 0.016794,
     "end_time": "2025-05-04T18:26:36.705607",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.688813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionEmbeddingRandom(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding using random spatial frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_pos_feats: int = 64, scale: Optional[float] = None) -> None:\n",
    "        super().__init__()\n",
    "        if scale is None or scale <= 0.0:\n",
    "            scale = 1.0\n",
    "        self.register_buffer(\n",
    "            \"positional_encoding_gaussian_matrix\",\n",
    "            scale * torch.randn((2, num_pos_feats)),\n",
    "        )\n",
    "\n",
    "    def _pe_encoding(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n",
    "        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n",
    "        coords = 2 * coords - 1\n",
    "        coords = coords @ self.positional_encoding_gaussian_matrix\n",
    "        coords = 2 * np.pi * coords\n",
    "        # outputs d_1 x ... x d_n x C shape\n",
    "        return torch.cat([torch.sin(coords), torch.cos(coords)], dim=-1)\n",
    "\n",
    "    def forward(self, size: Tuple[int, int]) -> torch.Tensor:\n",
    "        \"\"\"Generate positional encoding for a grid of the specified size.\"\"\"\n",
    "        h, w = size\n",
    "        device: Any = self.positional_encoding_gaussian_matrix.device\n",
    "        grid = torch.ones((h, w), device=device, dtype=torch.float32)\n",
    "        y_embed = grid.cumsum(dim=0) - 0.5\n",
    "        x_embed = grid.cumsum(dim=1) - 0.5\n",
    "        y_embed = y_embed / h\n",
    "        x_embed = x_embed / w\n",
    "\n",
    "        pe = self._pe_encoding(torch.stack([x_embed, y_embed], dim=-1))\n",
    "        return pe.permute(2, 0, 1)  # C x H x W\n",
    "\n",
    "    def forward_with_coords(\n",
    "        self, coords_input: torch.Tensor, image_size: Tuple[int, int]\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Positionally encode points that are not normalized to [0,1].\"\"\"\n",
    "        coords = coords_input.clone()\n",
    "        coords[:, :, 0] = coords[:, :, 0] / image_size[1]\n",
    "        coords[:, :, 1] = coords[:, :, 1] / image_size[0]\n",
    "        return self._pe_encoding(coords.to(torch.float))  # B x N x C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d491fde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:36.722343Z",
     "iopub.status.busy": "2025-05-04T18:26:36.722107Z",
     "iopub.status.idle": "2025-05-04T18:26:36.726022Z",
     "shell.execute_reply": "2025-05-04T18:26:36.725482Z"
    },
    "papermill": {
     "duration": 0.013595,
     "end_time": "2025-05-04T18:26:36.727150",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.713555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FaceXFormerMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(input_dim, 256)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        hidden_states = hidden_states.flatten(2).transpose(1, 2)\n",
    "        hidden_states = self.proj(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "416ce459",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:36.743895Z",
     "iopub.status.busy": "2025-05-04T18:26:36.743682Z",
     "iopub.status.idle": "2025-05-04T18:26:36.753002Z",
     "shell.execute_reply": "2025-05-04T18:26:36.752314Z"
    },
    "papermill": {
     "duration": 0.019159,
     "end_time": "2025-05-04T18:26:36.754257",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.735098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FaceXFormer(nn.Module):\n",
    "\n",
    "    def __init__(self, backbone_name: str):\n",
    "        super(FaceXFormer, self).__init__()\n",
    "\n",
    "        if backbone_name != 'swin_b':\n",
    "            raise ValueError(f\"Unsupported backbone name: {backbone_name}\")\n",
    "\n",
    "        swin_v2 = swin_b(weights='IMAGENET1K_V1')\n",
    "        self.backbone = torch.nn.Sequential(*(list(swin_v2.children())[:-1]))\n",
    "        self.target_layer_names = ['0.1', '0.3', '0.5', '0.7']\n",
    "        self.multi_scale_features = []\n",
    "\n",
    "        embed_dim = 1024\n",
    "        out_chans = 256\n",
    "\n",
    "        self.pe_layer = PositionEmbeddingRandom(out_chans // 2)\n",
    "\n",
    "        for name, module in self.backbone.named_modules():\n",
    "            if name in self.target_layer_names:\n",
    "                module.register_forward_hook(self.save_features_hook(name))\n",
    "\n",
    "        self.face_decoder = FaceDecoder(\n",
    "            transformer_dim=256,\n",
    "            transformer=TwoWayTransformer(\n",
    "                depth=2,\n",
    "                embedding_dim=256,\n",
    "                mlp_dim=2048,\n",
    "                num_heads=8,\n",
    "            ))\n",
    "\n",
    "        num_encoder_blocks = 4\n",
    "        hidden_sizes = [128, 256, 512, 1024]\n",
    "        decoder_hidden_size = 256\n",
    "\n",
    "        mlps = []\n",
    "        for i in range(num_encoder_blocks):\n",
    "            mlp = FaceXFormerMLP(input_dim=hidden_sizes[i])\n",
    "            mlps.append(mlp)\n",
    "        self.linear_c = nn.ModuleList(mlps)\n",
    "\n",
    "        self.linear_fuse = nn.Conv2d(\n",
    "            in_channels=decoder_hidden_size * num_encoder_blocks,\n",
    "            out_channels=decoder_hidden_size,\n",
    "            kernel_size=1,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "    def save_features_hook(self, name):\n",
    "        def hook(module, input, output):\n",
    "            self.multi_scale_features.append(\n",
    "                output.permute(0, 3, 1, 2).contiguous())\n",
    "        return hook\n",
    "\n",
    "    def forward(self, x, labels, tasks):\n",
    "        self.multi_scale_features.clear()\n",
    "\n",
    "        _, _, h, w = x.shape\n",
    "        features = self.backbone(x).squeeze()\n",
    "\n",
    "        batch_size = self.multi_scale_features[-1].shape[0]\n",
    "        all_hidden_states = ()\n",
    "        for encoder_hidden_state, mlp in zip(self.multi_scale_features, self.linear_c):\n",
    "\n",
    "            height, width = encoder_hidden_state.shape[2], encoder_hidden_state.shape[3]\n",
    "            encoder_hidden_state = mlp(encoder_hidden_state)\n",
    "            encoder_hidden_state = encoder_hidden_state.permute(0, 2, 1)\n",
    "            encoder_hidden_state = encoder_hidden_state.reshape(\n",
    "                batch_size, -1, height, width)\n",
    "            encoder_hidden_state = nn.functional.interpolate(\n",
    "                encoder_hidden_state, size=self.multi_scale_features[0].size()[\n",
    "                    2:],\n",
    "                mode=\"bilinear\", align_corners=False\n",
    "            )\n",
    "            all_hidden_states += (encoder_hidden_state,)\n",
    "\n",
    "        fused_states = self.linear_fuse(\n",
    "            torch.cat(all_hidden_states[::-1], dim=1))\n",
    "        image_pe = self.pe_layer(\n",
    "            (fused_states.shape[2], fused_states.shape[3])).unsqueeze(0)\n",
    "\n",
    "        landmark_output = self.face_decoder(\n",
    "            image_embeddings=fused_states,\n",
    "            image_pe=image_pe\n",
    "        )\n",
    "\n",
    "        return landmark_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0173ab69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:36.815403Z",
     "iopub.status.busy": "2025-05-04T18:26:36.814854Z",
     "iopub.status.idle": "2025-05-04T18:26:36.818628Z",
     "shell.execute_reply": "2025-05-04T18:26:36.818006Z"
    },
    "papermill": {
     "duration": 0.057765,
     "end_time": "2025-05-04T18:26:36.819698",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.761933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [3] After defining FaceXFormer model!!!!\n"
     ]
    }
   ],
   "source": [
    "# [LOG]\n",
    "print(\">> [3] After defining FaceXFormer model!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd40d1",
   "metadata": {
    "papermill": {
     "duration": 0.008095,
     "end_time": "2025-05-04T18:26:36.836555",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.828460",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Loss functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db08324c",
   "metadata": {
    "papermill": {
     "duration": 0.007855,
     "end_time": "2025-05-04T18:26:36.852412",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.844557",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.1. WingLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64c5039d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:36.869516Z",
     "iopub.status.busy": "2025-05-04T18:26:36.869265Z",
     "iopub.status.idle": "2025-05-04T18:26:36.873967Z",
     "shell.execute_reply": "2025-05-04T18:26:36.873460Z"
    },
    "papermill": {
     "duration": 0.014349,
     "end_time": "2025-05-04T18:26:36.874962",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.860613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wing Loss\n",
    "class WingLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, omega=0.01, epsilon=2):\n",
    "        super(WingLoss, self).__init__()\n",
    "        self.omega = omega\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        y = target\n",
    "        y_hat = pred\n",
    "        delta_2 = (y - y_hat).pow(2).sum(dim=-1, keepdim=False)\n",
    "        # delta = delta_2.sqrt()\n",
    "        delta = delta_2.clamp(min=1e-6).sqrt()\n",
    "        C = self.omega - self.omega * math.log(1 + self.omega / self.epsilon)\n",
    "        loss = torch.where(\n",
    "            delta < self.omega,\n",
    "            self.omega * torch.log(1 + delta / self.epsilon),\n",
    "            delta - C\n",
    "        )\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c895956d",
   "metadata": {
    "papermill": {
     "duration": 0.007899,
     "end_time": "2025-05-04T18:26:36.891551",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.883652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.2. SmoothL1Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc26660c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:36.908541Z",
     "iopub.status.busy": "2025-05-04T18:26:36.908352Z",
     "iopub.status.idle": "2025-05-04T18:26:36.913802Z",
     "shell.execute_reply": "2025-05-04T18:26:36.913230Z"
    },
    "papermill": {
     "duration": 0.015173,
     "end_time": "2025-05-04T18:26:36.914899",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.899726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Smooth L1 Loss\n",
    "class SmoothL1Loss(nn.Module):\n",
    "\n",
    "    def __init__(self, scale=0.01):\n",
    "        super(SmoothL1Loss, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.EPSILON = 1e-10\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"SmoothL1Loss()\"\n",
    "\n",
    "    def forward(self, output: torch.Tensor, groundtruth: torch.Tensor, reduction='mean'):\n",
    "        \"\"\"\n",
    "            input:  b x n x 2\n",
    "            output: b x n x 1 => 1\n",
    "        \"\"\"\n",
    "        if output.dim() == 4:\n",
    "            shape = output.shape\n",
    "            groundtruth = groundtruth.reshape(shape[0], shape[1], 1, shape[3])\n",
    "\n",
    "        delta_2 = (output - groundtruth).pow(2).sum(dim=-1, keepdim=False)\n",
    "        delta = delta_2.clamp(min=1e-6).sqrt()\n",
    "        # delta = torch.sqrt(delta_2 + self.EPSILON)\n",
    "        loss = torch.where(\n",
    "            delta_2 < self.scale * self.scale,\n",
    "            0.5 / self.scale * delta_2,\n",
    "            delta - 0.5 * self.scale)\n",
    "\n",
    "        if reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        elif reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48c9953",
   "metadata": {
    "papermill": {
     "duration": 0.008088,
     "end_time": "2025-05-04T18:26:36.931133",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.923045",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.3. AWingLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee8c99ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:36.948518Z",
     "iopub.status.busy": "2025-05-04T18:26:36.948260Z",
     "iopub.status.idle": "2025-05-04T18:26:36.955388Z",
     "shell.execute_reply": "2025-05-04T18:26:36.954811Z"
    },
    "papermill": {
     "duration": 0.017103,
     "end_time": "2025-05-04T18:26:36.956501",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.939398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AWingLoss\n",
    "class AWingLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, omega=14, theta=0.5, epsilon=1, alpha=2.1, use_weight_map=True):\n",
    "        super(AWingLoss, self).__init__()\n",
    "        self.omega = omega\n",
    "        self.theta = theta\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.use_weight_map = use_weight_map\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"AWingLoss()\"\n",
    "\n",
    "    def generate_weight_map(self, heatmap, k_size=3, w=10):\n",
    "        dilate = F.max_pool2d(heatmap, kernel_size=k_size, stride=1, padding=1)\n",
    "        weight_map = torch.where(dilate < 0.2, torch.zeros_like(\n",
    "            heatmap), torch.ones_like(heatmap))\n",
    "        return w * weight_map + 1\n",
    "\n",
    "    def forward(self, output, groundtruth):\n",
    "        \"\"\"\n",
    "        input:  b x n x h x w\n",
    "        output: b x n x h x w => 1\n",
    "        \"\"\"\n",
    "        delta = (output - groundtruth).abs()\n",
    "        A = self.omega * (1 / (1 + torch.pow(self.theta / self.epsilon, self.alpha - groundtruth))) * (self.alpha - groundtruth) * \\\n",
    "            (torch.pow(self.theta / self.epsilon, self.alpha -\n",
    "             groundtruth - 1)) * (1 / self.epsilon)\n",
    "        C = self.theta * A - self.omega * \\\n",
    "            torch.log(1 + torch.pow(self.theta /\n",
    "                      self.epsilon, self.alpha - groundtruth))\n",
    "        loss = torch.where(delta < self.theta,\n",
    "                           self.omega *\n",
    "                           torch.log(\n",
    "                               1 + torch.pow(delta / self.epsilon, self.alpha - groundtruth)),\n",
    "                           (A * delta - C))\n",
    "        if self.use_weight_map:\n",
    "            weight = self.generate_weight_map(groundtruth)\n",
    "            loss = loss * weight\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd39582",
   "metadata": {
    "papermill": {
     "duration": 0.007711,
     "end_time": "2025-05-04T18:26:36.972509",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.964798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.4. STARLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8575998",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:36.990660Z",
     "iopub.status.busy": "2025-05-04T18:26:36.990454Z",
     "iopub.status.idle": "2025-05-04T18:26:37.006753Z",
     "shell.execute_reply": "2025-05-04T18:26:37.006024Z"
    },
    "papermill": {
     "duration": 0.026504,
     "end_time": "2025-05-04T18:26:37.007893",
     "exception": false,
     "start_time": "2025-05-04T18:26:36.981389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STAR Loss\n",
    "def get_channel_sum(input):\n",
    "    temp = torch.sum(input, dim=3)\n",
    "    output = torch.sum(temp, dim=2)\n",
    "    return output\n",
    "\n",
    "\n",
    "def expand_two_dimensions_at_end(input, dim1, dim2):\n",
    "    input = input.unsqueeze(-1).unsqueeze(-1)\n",
    "    input = input.expand(-1, -1, dim1, dim2)\n",
    "    return input\n",
    "\n",
    "\n",
    "class STARLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, w=1, dist='smoothl1', num_dim_image=2, EPSILON=1e-5):\n",
    "        super(STARLoss, self).__init__()\n",
    "        self.w = w\n",
    "        self.num_dim_image = num_dim_image\n",
    "        self.EPSILON = EPSILON\n",
    "        self.dist = dist\n",
    "        if self.dist == 'smoothl1':\n",
    "            self.dist_func = SmoothL1Loss()\n",
    "        elif self.dist == 'l1':\n",
    "            self.dist_func = F.l1_loss\n",
    "        elif self.dist == 'l2':\n",
    "            self.dist_func = F.mse_loss\n",
    "        elif self.dist == 'wing':\n",
    "            self.dist_func = WingLoss()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"STARLoss()\"\n",
    "\n",
    "    def _make_grid(self, h, w):\n",
    "        yy, xx = torch.meshgrid(\n",
    "            torch.arange(h).float() / (h - 1) * 2 - 1,\n",
    "            torch.arange(w).float() / (w - 1) * 2 - 1)\n",
    "        return yy, xx\n",
    "\n",
    "    def weighted_mean(self, heatmap):\n",
    "        batch, npoints, h, w = heatmap.shape\n",
    "\n",
    "        yy, xx = self._make_grid(h, w)\n",
    "        yy = yy.view(1, 1, h, w).to(heatmap)\n",
    "        xx = xx.view(1, 1, h, w).to(heatmap)\n",
    "\n",
    "        yy_coord = (yy * heatmap).sum([2, 3])  # batch x npoints\n",
    "        xx_coord = (xx * heatmap).sum([2, 3])  # batch x npoints\n",
    "        coords = torch.stack([xx_coord, yy_coord], dim=-1)\n",
    "        return coords\n",
    "\n",
    "    def unbiased_weighted_covariance(self, htp, means, num_dim_image=2, EPSILON=1e-5):\n",
    "        batch_size, num_points, height, width = htp.shape\n",
    "\n",
    "        yv, xv = self._make_grid(height, width)\n",
    "        xv = Variable(xv)\n",
    "        yv = Variable(yv)\n",
    "\n",
    "        if htp.is_cuda:\n",
    "            xv = xv.cuda()\n",
    "            yv = yv.cuda()\n",
    "\n",
    "        xmean = means[:, :, 0]\n",
    "        xv_minus_mean = xv.expand(batch_size, num_points, -1, -1) - expand_two_dimensions_at_end(xmean, height,\n",
    "                                                                                                 width)  # [batch_size, 68, 64, 64]\n",
    "        ymean = means[:, :, 1]\n",
    "        yv_minus_mean = yv.expand(batch_size, num_points, -1, -1) - expand_two_dimensions_at_end(ymean, height,\n",
    "                                                                                                 width)  # [batch_size, 68, 64, 64]\n",
    "        wt_xv_minus_mean = xv_minus_mean\n",
    "        wt_yv_minus_mean = yv_minus_mean\n",
    "\n",
    "        wt_xv_minus_mean = wt_xv_minus_mean.view(\n",
    "            batch_size * num_points, height * width)  # [batch_size*68, 4096]\n",
    "        wt_xv_minus_mean = wt_xv_minus_mean.view(\n",
    "            batch_size * num_points, 1, height * width)  # [batch_size*68, 1, 4096]\n",
    "        wt_yv_minus_mean = wt_yv_minus_mean.view(\n",
    "            batch_size * num_points, height * width)  # [batch_size*68, 4096]\n",
    "        wt_yv_minus_mean = wt_yv_minus_mean.view(\n",
    "            batch_size * num_points, 1, height * width)  # [batch_size*68, 1, 4096]\n",
    "        # [batch_size*68, 2, 4096]\n",
    "        vec_concat = torch.cat((wt_xv_minus_mean, wt_yv_minus_mean), 1)\n",
    "\n",
    "        htp_vec = htp.view(batch_size * num_points, 1, height * width)\n",
    "        htp_vec = htp_vec.expand(-1, 2, -1)\n",
    "\n",
    "        # [batch_size*68, 2, 2]\n",
    "        covariance = torch.bmm(htp_vec * vec_concat,\n",
    "                               vec_concat.transpose(1, 2))\n",
    "        covariance = covariance.view(\n",
    "            batch_size, num_points, num_dim_image, num_dim_image)  # [batch_size, 68, 2, 2]\n",
    "\n",
    "        V_1 = htp.sum([2, 3]) + EPSILON  # [batch_size, 68]\n",
    "        V_2 = torch.pow(htp, 2).sum([2, 3]) + EPSILON  # [batch_size, 68]\n",
    "\n",
    "        denominator = V_1 - (V_2 / V_1)\n",
    "        covariance = covariance / \\\n",
    "            expand_two_dimensions_at_end(\n",
    "                denominator, num_dim_image, num_dim_image)\n",
    "\n",
    "        return covariance\n",
    "\n",
    "    def ambiguity_guided_decompose(self, pts, eigenvalues, eigenvectors):\n",
    "        batch_size, npoints = pts.shape[:2]\n",
    "        rotate = torch.matmul(\n",
    "            pts.view(batch_size, npoints, 1, 2), eigenvectors.transpose(-1, -2))\n",
    "        scale = rotate.view(batch_size, npoints, 2) / \\\n",
    "            torch.sqrt(eigenvalues + self.EPSILON)\n",
    "        return scale\n",
    "\n",
    "    def eigenvalue_restriction(self, evalues, batch, npoints):\n",
    "        eigen_loss = torch.abs(evalues.view(batch * npoints, 2)).sum(-1)\n",
    "        return eigen_loss.mean()\n",
    "\n",
    "    def forward(self, heatmap, groundtruth):\n",
    "        \"\"\"\n",
    "            heatmap:     b x n x 64 x 64\n",
    "            groundtruth: b x n x 2\n",
    "            output:      b x n x 1 => 1\n",
    "        \"\"\"\n",
    "        # normalize\n",
    "        bs, npoints, h, w = heatmap.shape\n",
    "        heatmap_sum = torch.clamp(heatmap.sum([2, 3]), min=1e-6)\n",
    "        heatmap = heatmap / heatmap_sum.view(bs, npoints, 1, 1)\n",
    "\n",
    "        means = self.weighted_mean(heatmap)  # [bs, 68, 2]\n",
    "        covars = self.unbiased_weighted_covariance(\n",
    "            heatmap, means)  # covars [bs, 68, 2, 2]\n",
    "\n",
    "        # TODO: GPU-based eigen-decomposition\n",
    "        # https://github.com/pytorch/pytorch/issues/60537\n",
    "        _covars = covars.view(bs * npoints, 2, 2).cpu()\n",
    "        # evalues [bs * 68, 2], evectors [bs * 68, 2, 2]\n",
    "        evalues, evectors = torch.linalg.eigh(_covars, UPLO='L')\n",
    "        evalues = evalues.view(bs, npoints, 2).to(heatmap)\n",
    "        evectors = evectors.view(bs, npoints, 2, 2).to(heatmap)\n",
    "\n",
    "        # STAR Loss\n",
    "        # Ambiguity-guided Decomposition\n",
    "        error = self.ambiguity_guided_decompose(\n",
    "            groundtruth - means, evalues, evectors)\n",
    "        loss_trans = self.dist_func(torch.zeros_like(error).to(error), error)\n",
    "        # Eigenvalue Restriction\n",
    "        loss_eigen = self.eigenvalue_restriction(evalues, bs, npoints)\n",
    "        star_loss = loss_trans + self.w * loss_eigen\n",
    "\n",
    "        return star_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6081fa56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:37.025409Z",
     "iopub.status.busy": "2025-05-04T18:26:37.025164Z",
     "iopub.status.idle": "2025-05-04T18:26:37.028861Z",
     "shell.execute_reply": "2025-05-04T18:26:37.028231Z"
    },
    "papermill": {
     "duration": 0.013487,
     "end_time": "2025-05-04T18:26:37.029855",
     "exception": false,
     "start_time": "2025-05-04T18:26:37.016368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [4] After defining loss functions!!!!\n"
     ]
    }
   ],
   "source": [
    "# [LOG]\n",
    "print(\">> [4] After defining loss functions!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaf7541",
   "metadata": {
    "papermill": {
     "duration": 0.008352,
     "end_time": "2025-05-04T18:26:37.047996",
     "exception": false,
     "start_time": "2025-05-04T18:26:37.039644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6cb0b6",
   "metadata": {
    "papermill": {
     "duration": 0.008125,
     "end_time": "2025-05-04T18:26:37.064616",
     "exception": false,
     "start_time": "2025-05-04T18:26:37.056491",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.1. Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b740875f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:37.081478Z",
     "iopub.status.busy": "2025-05-04T18:26:37.081219Z",
     "iopub.status.idle": "2025-05-04T18:26:37.085960Z",
     "shell.execute_reply": "2025-05-04T18:26:37.085330Z"
    },
    "papermill": {
     "duration": 0.014395,
     "end_time": "2025-05-04T18:26:37.086947",
     "exception": false,
     "start_time": "2025-05-04T18:26:37.072552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "class Accuracy:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Accuracy()\"\n",
    "\n",
    "    def test(self, label_pd, label_gt, ignore_label=-1):\n",
    "        correct_cnt = 0\n",
    "        total_cnt = 0\n",
    "        with torch.no_grad():\n",
    "            label_pd = F.softmax(label_pd, dim=1)\n",
    "            label_pd = torch.max(label_pd, 1)[1]\n",
    "            label_gt = label_gt.long()\n",
    "            c = (label_pd == label_gt)\n",
    "            correct_cnt = torch.sum(c).item()\n",
    "            total_cnt = c.size(0) - torch.sum(label_gt==ignore_label).item()\n",
    "        return correct_cnt, total_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb59b6ee",
   "metadata": {
    "papermill": {
     "duration": 0.007901,
     "end_time": "2025-05-04T18:26:37.102995",
     "exception": false,
     "start_time": "2025-05-04T18:26:37.095094",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.2. FR_AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ee086e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:37.120547Z",
     "iopub.status.busy": "2025-05-04T18:26:37.120264Z",
     "iopub.status.idle": "2025-05-04T18:26:37.125216Z",
     "shell.execute_reply": "2025-05-04T18:26:37.124739Z"
    },
    "papermill": {
     "duration": 0.014923,
     "end_time": "2025-05-04T18:26:37.126330",
     "exception": false,
     "start_time": "2025-05-04T18:26:37.111407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FR_AUC\n",
    "class FR_AUC:\n",
    "    \n",
    "    def __init__(self, data_definition):\n",
    "        self.data_definition = data_definition\n",
    "        if data_definition == '300W':\n",
    "            self.thresh = 0.05\n",
    "        else:\n",
    "            self.thresh = 0.1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"FR_AUC()\"\n",
    "\n",
    "    def test(self, nmes, thres=None, step=0.0001):\n",
    "        if thres is None:\n",
    "            thres = self.thresh\n",
    "\n",
    "        num_data = len(nmes)\n",
    "        xs = np.arange(0, thres + step, step)\n",
    "        ys = np.array([np.count_nonzero(nmes <= x) for x in xs]) / float(num_data)\n",
    "        fr = 1.0 - ys[-1]\n",
    "        auc = simps(ys, x=xs) / thres\n",
    "        return [round(fr, 4), round(auc, 6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438cad2",
   "metadata": {
    "papermill": {
     "duration": 0.00862,
     "end_time": "2025-05-04T18:26:37.143176",
     "exception": false,
     "start_time": "2025-05-04T18:26:37.134556",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.3. NME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31dcb6c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:37.161207Z",
     "iopub.status.busy": "2025-05-04T18:26:37.161002Z",
     "iopub.status.idle": "2025-05-04T18:26:37.167333Z",
     "shell.execute_reply": "2025-05-04T18:26:37.166790Z"
    },
    "papermill": {
     "duration": 0.016716,
     "end_time": "2025-05-04T18:26:37.168440",
     "exception": false,
     "start_time": "2025-05-04T18:26:37.151724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normalized Mean Error\n",
    "class NME:\n",
    "    \n",
    "    def __init__(self, nme_left_index, nme_right_index):\n",
    "        self.nme_left_index = nme_left_index\n",
    "        self.nme_right_index = nme_right_index\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"NME()\"\n",
    "\n",
    "    def get_norm_distance(self, landmarks):\n",
    "        assert isinstance(self.nme_right_index, list), 'the nme_right_index is not list.'\n",
    "        assert isinstance(self.nme_left_index, list), 'the nme_left, index is not list.'\n",
    "        right_pupil = landmarks[self.nme_right_index, :].mean(0)\n",
    "        left_pupil = landmarks[self.nme_left_index, :].mean(0)\n",
    "        norm_distance = np.linalg.norm(right_pupil - left_pupil)\n",
    "        return norm_distance\n",
    "\n",
    "    def test(self, label_pd, label_gt):\n",
    "        nme_list = []\n",
    "        label_pd = label_pd.data.cpu().numpy()\n",
    "        label_gt = label_gt.data.cpu().numpy()\n",
    "\n",
    "        for i in range(label_gt.shape[0]):\n",
    "            landmarks_gt = label_gt[i]\n",
    "            landmarks_pv = label_pd[i]\n",
    "            if isinstance(self.nme_right_index, list):\n",
    "                norm_distance = self.get_norm_distance(landmarks_gt)\n",
    "            elif isinstance(self.nme_right_index, int):\n",
    "                norm_distance = np.linalg.norm(landmarks_gt[self.nme_left_index] - landmarks_gt[self.nme_right_index])\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            landmarks_delta = landmarks_pv - landmarks_gt\n",
    "            nme = (np.linalg.norm(landmarks_delta, axis=1) / norm_distance).mean()\n",
    "            nme_list.append(nme)\n",
    "            # sum_nme += nme\n",
    "            # total_cnt += 1\n",
    "        return nme_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50482f57",
   "metadata": {
    "papermill": {
     "duration": 0.008904,
     "end_time": "2025-05-04T18:26:37.186235",
     "exception": false,
     "start_time": "2025-05-04T18:26:37.177331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.4. count_parameters_in_MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e86896b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:37.205348Z",
     "iopub.status.busy": "2025-05-04T18:26:37.205108Z",
     "iopub.status.idle": "2025-05-04T18:26:37.208820Z",
     "shell.execute_reply": "2025-05-04T18:26:37.208306Z"
    },
    "papermill": {
     "duration": 0.014743,
     "end_time": "2025-05-04T18:26:37.209942",
     "exception": false,
     "start_time": "2025-05-04T18:26:37.195199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count parameters in MB\n",
    "def count_parameters_in_MB(model):\n",
    "    if isinstance(model, nn.Module):\n",
    "        return sum(v.numel() for v in model.parameters()) / 1e6\n",
    "    else:\n",
    "        return sum(v.numel() for v in model) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "009001f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:37.227634Z",
     "iopub.status.busy": "2025-05-04T18:26:37.227431Z",
     "iopub.status.idle": "2025-05-04T18:26:37.230845Z",
     "shell.execute_reply": "2025-05-04T18:26:37.230155Z"
    },
    "papermill": {
     "duration": 0.013314,
     "end_time": "2025-05-04T18:26:37.231841",
     "exception": false,
     "start_time": "2025-05-04T18:26:37.218527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [5] After defining metrics!!!!\n"
     ]
    }
   ],
   "source": [
    "# [LOG]\n",
    "print(\">> [5] After defining metrics!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c1f5c2",
   "metadata": {
    "papermill": {
     "duration": 0.008534,
     "end_time": "2025-05-04T18:26:37.248927",
     "exception": false,
     "start_time": "2025-05-04T18:26:37.240393",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a9723c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:37.266820Z",
     "iopub.status.busy": "2025-05-04T18:26:37.266612Z",
     "iopub.status.idle": "2025-05-04T18:26:37.274086Z",
     "shell.execute_reply": "2025-05-04T18:26:37.273537Z"
    },
    "papermill": {
     "duration": 0.017744,
     "end_time": "2025-05-04T18:26:37.275210",
     "exception": false,
     "start_time": "2025-05-04T18:26:37.257466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Face300WXMLDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset cho iBUG 300-W với XML annotations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, xml_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        xml_path = os.path.join(root_dir, xml_file)\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        for img in root.findall('images/image'):\n",
    "            rel_path = img.get('file')\n",
    "            img_path = os.path.join(root_dir, rel_path)\n",
    "            parts = img.find('box').findall('part')\n",
    "            lm = np.zeros((len(parts), 2), dtype=np.float16)\n",
    "            for p in parts:\n",
    "                idx = int(p.get('name'))\n",
    "                lm[idx, 0] = np.float16(p.get('x'))\n",
    "                lm[idx, 1] = np.float16(p.get('y'))\n",
    "            self.samples.append({\"img_path\": img_path, \"landmarks\": lm})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        image = Image.open(sample['img_path']).convert('RGB')\n",
    "        landmarks = torch.from_numpy(sample['landmarks'])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return {\"image\": image, \"landmarks\": landmarks}\n",
    "\n",
    "    # Apply the transform to all samples\n",
    "    def apply_transform(self, transform) -> None:\n",
    "        for sample in self.samples:\n",
    "            image = Image.open(sample['img_path']).convert('RGB')\n",
    "            sample['image'] = transform(image)\n",
    "            sample['landmarks'] = torch.from_numpy(sample['landmarks']).float()\n",
    "\n",
    "    # Set the transform for the dataset\n",
    "    def set_transform(self, transform):\n",
    "        self.transform = transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5cf09f",
   "metadata": {
    "papermill": {
     "duration": 0.008154,
     "end_time": "2025-05-04T18:26:37.291967",
     "exception": false,
     "start_time": "2025-05-04T18:26:37.283813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6.1. Định nghĩa các transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfcebbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:37.309307Z",
     "iopub.status.busy": "2025-05-04T18:26:37.309090Z",
     "iopub.status.idle": "2025-05-04T18:26:37.314834Z",
     "shell.execute_reply": "2025-05-04T18:26:37.314326Z"
    },
    "papermill": {
     "duration": 0.015551,
     "end_time": "2025-05-04T18:26:37.315777",
     "exception": false,
     "start_time": "2025-05-04T18:26:37.300226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training transforms với augmentations\n",
    "train_transform = transforms.Compose([\n",
    "    # Resize về 224×224\n",
    "    # ⚙️ độ phân giải đầu vào :contentReference[oaicite:7]{index=7}\n",
    "    transforms.Resize(size=(224,224), interpolation=InterpolationMode.BICUBIC),\n",
    "\n",
    "    # Random affine: rotation ±18°, translate ±5%, scale [0.9,1.1]\n",
    "    transforms.RandomAffine(\n",
    "        # ±18° :contentReference[oaicite:8]{index=8}\n",
    "        degrees=18,\n",
    "        # ±5% kích thước :contentReference[oaicite:9]{index=9}\n",
    "        translate=(0.05, 0.05),\n",
    "        # ±10% scale :contentReference[oaicite:10]{index=10}\n",
    "        scale=(0.9, 1.1)\n",
    "    ),\n",
    "\n",
    "    # Horizontal flip 50%\n",
    "    # :contentReference[oaicite:11]{index=11}\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "    # Grayscale chuyển 20%\n",
    "    # :contentReference[oaicite:12]{index=12}\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "\n",
    "    # Gaussian blur 30%\n",
    "    transforms.RandomApply(\n",
    "        # or (3,3) :contentReference[oaicite:13]{index=13}\n",
    "        [transforms.GaussianBlur(kernel_size=5)],\n",
    "        p=0.3\n",
    "    ),\n",
    "\n",
    "    # Gamma adjustment 20%\n",
    "    transforms.RandomApply(\n",
    "        [transforms.Lambda(lambda img: transforms.functional.adjust_gamma(img,\n",
    "                                                                          gamma=np.random.uniform(0.8, 1.2)))],\n",
    "        p=0.2\n",
    "    ),                                        # :contentReference[oaicite:14]{index=14}\n",
    "\n",
    "    # Occlusion: RandomErasing 40%\n",
    "    transforms.RandomErasing(p=0.4,\n",
    "                             scale=(0.02, 0.25),\n",
    "                             ratio=(0.3, 3.3),\n",
    "                             value='random'),   # :contentReference[oaicite:15]{index=15}\n",
    "\n",
    "    # Cuối cùng: ToTensor + Normalize ImageNet\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f375f81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:37.332640Z",
     "iopub.status.busy": "2025-05-04T18:26:37.332404Z",
     "iopub.status.idle": "2025-05-04T18:26:37.336356Z",
     "shell.execute_reply": "2025-05-04T18:26:37.335626Z"
    },
    "papermill": {
     "duration": 0.013927,
     "end_time": "2025-05-04T18:26:37.337637",
     "exception": false,
     "start_time": "2025-05-04T18:26:37.323710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec5e078",
   "metadata": {
    "papermill": {
     "duration": 0.008426,
     "end_time": "2025-05-04T18:26:37.354652",
     "exception": false,
     "start_time": "2025-05-04T18:26:37.346226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6.2. Khởi tạo DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5aa73a14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:37.372887Z",
     "iopub.status.busy": "2025-05-04T18:26:37.372237Z",
     "iopub.status.idle": "2025-05-04T18:26:41.222733Z",
     "shell.execute_reply": "2025-05-04T18:26:41.222138Z"
    },
    "papermill": {
     "duration": 3.861033,
     "end_time": "2025-05-04T18:26:41.224125",
     "exception": false,
     "start_time": "2025-05-04T18:26:37.363092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Đường dẫn dataset\n",
    "if os.path.exists('./ibug_300W_large_face_landmark_dataset'):\n",
    "    root = './ibug_300W_large_face_landmark_dataset'\n",
    "else:\n",
    "    root = '/kaggle/input/ibug-300w-large-face-landmark-dataset/ibug_300W_large_face_landmark_dataset'\n",
    "train_xml = 'labels_ibug_300W_train.xml'\n",
    "test_xml = 'labels_ibug_300W_test.xml'\n",
    "\n",
    "# Dataset\n",
    "full_train_ds = Face300WXMLDataset(root, train_xml, transform=train_transform)\n",
    "test_ds = Face300WXMLDataset(root, test_xml, transform=val_test_transform)\n",
    "\n",
    "# Chia train/val 90/10, seed cố định\n",
    "val_count = int(0.1 * len(full_train_ds))\n",
    "train_count = len(full_train_ds) - val_count\n",
    "generator = torch.Generator().manual_seed(0)\n",
    "train_ds, val_ds = random_split(full_train_ds,\n",
    "                                [train_count, val_count],\n",
    "                                generator=generator)\n",
    "\n",
    "# Set transform cho train/val/test\n",
    "train_ds.dataset.set_transform(train_transform)\n",
    "val_ds.dataset.set_transform(val_test_transform)\n",
    "test_ds.set_transform(val_test_transform)\n",
    "\n",
    "# DataLoader với batch_size=48\n",
    "train_loader = DataLoader(train_ds, batch_size=48, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=48, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=48, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a1bfe4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:41.242745Z",
     "iopub.status.busy": "2025-05-04T18:26:41.242078Z",
     "iopub.status.idle": "2025-05-04T18:26:44.342650Z",
     "shell.execute_reply": "2025-05-04T18:26:44.341754Z"
    },
    "papermill": {
     "duration": 3.111024,
     "end_time": "2025-05-04T18:26:44.344058",
     "exception": false,
     "start_time": "2025-05-04T18:26:41.233034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch: images torch.Size([48, 3, 224, 224]), landmarks torch.Size([48, 68, 2])\n",
      "Train dataset size: 6000\n",
      "Validation dataset size: 666\n",
      "Test dataset size: 1008\n"
     ]
    }
   ],
   "source": [
    "# Iterate\n",
    "for batch in train_loader:\n",
    "    imgs = batch['image']       # (B,3,224,224)\n",
    "    lms = batch['landmarks']    # (B,68,2)\n",
    "    print(f\"Train batch: images {imgs.shape}, landmarks {lms.shape}\")\n",
    "    break\n",
    "\n",
    "# Print number of samples in each dataset\n",
    "print(f\"Train dataset size: {len(train_ds)}\")\n",
    "print(f\"Validation dataset size: {len(val_ds)}\")\n",
    "print(f\"Test dataset size: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1b6d457",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:44.364647Z",
     "iopub.status.busy": "2025-05-04T18:26:44.364400Z",
     "iopub.status.idle": "2025-05-04T18:26:44.368501Z",
     "shell.execute_reply": "2025-05-04T18:26:44.367801Z"
    },
    "papermill": {
     "duration": 0.014275,
     "end_time": "2025-05-04T18:26:44.369596",
     "exception": false,
     "start_time": "2025-05-04T18:26:44.355321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [6] After defining dataset and dataloader!!!!\n"
     ]
    }
   ],
   "source": [
    "# [LOG]\n",
    "print(\">> [6] After defining dataset and dataloader!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd05cfeb",
   "metadata": {
    "papermill": {
     "duration": 0.008248,
     "end_time": "2025-05-04T18:26:44.386633",
     "exception": false,
     "start_time": "2025-05-04T18:26:44.378385",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 7. Training Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f05eb66",
   "metadata": {
    "papermill": {
     "duration": 0.008126,
     "end_time": "2025-05-04T18:26:44.403258",
     "exception": false,
     "start_time": "2025-05-04T18:26:44.395132",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7.1. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b47c8e9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:44.421325Z",
     "iopub.status.busy": "2025-05-04T18:26:44.421071Z",
     "iopub.status.idle": "2025-05-04T18:26:44.509482Z",
     "shell.execute_reply": "2025-05-04T18:26:44.508668Z"
    },
    "papermill": {
     "duration": 0.098815,
     "end_time": "2025-05-04T18:26:44.510626",
     "exception": false,
     "start_time": "2025-05-04T18:26:44.411811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 12\n",
    "batch_size = 48\n",
    "initial_lr = 1e-4\n",
    "weight_decay = 1e-5\n",
    "milestones = [6, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1952495d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:44.528797Z",
     "iopub.status.busy": "2025-05-04T18:26:44.528335Z",
     "iopub.status.idle": "2025-05-04T18:26:44.531982Z",
     "shell.execute_reply": "2025-05-04T18:26:44.531334Z"
    },
    "papermill": {
     "duration": 0.013586,
     "end_time": "2025-05-04T18:26:44.532981",
     "exception": false,
     "start_time": "2025-05-04T18:26:44.519395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Print the used device\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f6671",
   "metadata": {
    "papermill": {
     "duration": 0.007962,
     "end_time": "2025-05-04T18:26:44.549344",
     "exception": false,
     "start_time": "2025-05-04T18:26:44.541382",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7.2. Model, Losses, Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f139d92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:44.567205Z",
     "iopub.status.busy": "2025-05-04T18:26:44.566574Z",
     "iopub.status.idle": "2025-05-04T18:26:49.750537Z",
     "shell.execute_reply": "2025-05-04T18:26:49.749634Z"
    },
    "papermill": {
     "duration": 5.194529,
     "end_time": "2025-05-04T18:26:49.752295",
     "exception": false,
     "start_time": "2025-05-04T18:26:44.557766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/swin_b-68c6b09e.pth\" to /root/.cache/torch/hub/checkpoints/swin_b-68c6b09e.pth\n",
      "100%|██████████| 335M/335M [00:01<00:00, 187MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Backbone choice: 'mobilenet', 'resnet101', 'convnext_b', 'swin_b'\n",
    "def get_model(backbone_name='swin_b'):\n",
    "    model = FaceXFormer(backbone_name=backbone_name)\n",
    "\n",
    "    # Initialize the parameters\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            # Add seed for reproducibility\n",
    "            nn.init.xavier_uniform_(\n",
    "                p, generator=torch.Generator().manual_seed(0))\n",
    "        else:\n",
    "            nn.init.zeros_(p)\n",
    "\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "# Load the model\n",
    "backbone_name: str = 'swin_b'\n",
    "model = get_model(backbone_name=backbone_name)\n",
    "\n",
    "\n",
    "# Landmark loss\n",
    "landmark_loss_fn = STARLoss().to(device)\n",
    "\n",
    "# Metrics\n",
    "nme_metric = NME(nme_left_index=[36, 39],\n",
    "                 nme_right_index=[42, 45])  # eye landmarks for normalization\n",
    "# uses simps for AUC :contentReference[oaicite:3]{index=3}\n",
    "fr_auc_metric = FR_AUC(data_definition='300W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfdb7e6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:49.773163Z",
     "iopub.status.busy": "2025-05-04T18:26:49.772638Z",
     "iopub.status.idle": "2025-05-04T18:26:49.778891Z",
     "shell.execute_reply": "2025-05-04T18:26:49.778114Z"
    },
    "papermill": {
     "duration": 0.017834,
     "end_time": "2025-05-04T18:26:49.779967",
     "exception": false,
     "start_time": "2025-05-04T18:26:49.762133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA enabled with automatic mixed precision\n",
      "GPU memory allocated: 0.38 GB\n",
      "Model size: 93.17 MB\n"
     ]
    }
   ],
   "source": [
    "# Enable CUDA optimization for half precision (if available)\n",
    "if device.type == 'cuda':\n",
    "    # Enable autocast for mixed precision training\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    print(\"CUDA enabled with automatic mixed precision\")\n",
    "else:\n",
    "    scaler = None\n",
    "    print(\"Running on CPU, mixed precision not enabled\")\n",
    "\n",
    "# Report memory usage\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    print(f\"Model size: {count_parameters_in_MB(model):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146a1a16",
   "metadata": {
    "papermill": {
     "duration": 0.009286,
     "end_time": "2025-05-04T18:26:49.798719",
     "exception": false,
     "start_time": "2025-05-04T18:26:49.789433",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7.3. Optimizer and Scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6a9792d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:49.818172Z",
     "iopub.status.busy": "2025-05-04T18:26:49.817691Z",
     "iopub.status.idle": "2025-05-04T18:26:49.822821Z",
     "shell.execute_reply": "2025-05-04T18:26:49.822216Z"
    },
    "papermill": {
     "duration": 0.01593,
     "end_time": "2025-05-04T18:26:49.823825",
     "exception": false,
     "start_time": "2025-05-04T18:26:49.807895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=initial_lr,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "scheduler = MultiStepLR(optimizer, milestones=milestones, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6269275f",
   "metadata": {
    "papermill": {
     "duration": 0.00916,
     "end_time": "2025-05-04T18:26:49.842202",
     "exception": false,
     "start_time": "2025-05-04T18:26:49.833042",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7.4. Training & Validation Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c394def",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:49.862085Z",
     "iopub.status.busy": "2025-05-04T18:26:49.861611Z",
     "iopub.status.idle": "2025-05-04T18:26:49.865863Z",
     "shell.execute_reply": "2025-05-04T18:26:49.865228Z"
    },
    "papermill": {
     "duration": 0.015406,
     "end_time": "2025-05-04T18:26:49.866846",
     "exception": false,
     "start_time": "2025-05-04T18:26:49.851440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def denorm_points(points, h, w, align_corners=False):\n",
    "    if align_corners:\n",
    "        denorm_points = (points + 1) / 2 * torch.tensor([w - 1, h - 1], dtype=torch.float32).to(points).view(1, 1, 2)\n",
    "    else:\n",
    "        denorm_points = ((points + 1) * torch.tensor([w, h], dtype=torch.float32).to(points).view(1, 1, 2) - 1) / 2\n",
    "\n",
    "    return denorm_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad23a3e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:49.885842Z",
     "iopub.status.busy": "2025-05-04T18:26:49.885654Z",
     "iopub.status.idle": "2025-05-04T18:26:49.890655Z",
     "shell.execute_reply": "2025-05-04T18:26:49.890130Z"
    },
    "papermill": {
     "duration": 0.015685,
     "end_time": "2025-05-04T18:26:49.891667",
     "exception": false,
     "start_time": "2025-05-04T18:26:49.875982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def heatmap_to_coordinates(heatmaps):\n",
    "    \"\"\"\n",
    "    Convert heatmaps to landmark coordinates\n",
    "    \n",
    "    Args:\n",
    "        heatmaps: Tensor of shape [batch_size, num_landmarks, h, w] \n",
    "                 containing probability heatmaps for each landmark\n",
    "    \n",
    "    Returns:\n",
    "        coordinates: Tensor of shape [batch_size, num_landmarks, 2] with x,y coordinates in [-1,1] range\n",
    "    \"\"\"\n",
    "    batch_size, num_landmarks, h, w = heatmaps.shape\n",
    "    \n",
    "    # Create normalized coordinate grid\n",
    "    y_range = torch.linspace(-1, 1, h, device=heatmaps.device)\n",
    "    x_range = torch.linspace(-1, 1, w, device=heatmaps.device)\n",
    "    y_grid, x_grid = torch.meshgrid(y_range, x_range, indexing='ij')\n",
    "    \n",
    "    # Normalize heatmaps to sum to 1\n",
    "    heatmap_sum = torch.clamp(heatmaps.sum([-1, -2]), min=1e-6).view(batch_size, num_landmarks, 1, 1)\n",
    "    heatmaps_normalized = heatmaps / heatmap_sum\n",
    "    \n",
    "    # Compute expected x,y coordinates (weighted average)\n",
    "    x_coords = (heatmaps_normalized * x_grid).sum([-1, -2])  # [batch_size, num_landmarks]\n",
    "    y_coords = (heatmaps_normalized * y_grid).sum([-1, -2])  # [batch_size, num_landmarks]\n",
    "    \n",
    "    # Stack x,y coordinates\n",
    "    coordinates = torch.stack([x_coords, y_coords], dim=-1)  # [batch_size, num_landmarks, 2]\n",
    "    \n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c27ef93a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:26:49.911014Z",
     "iopub.status.busy": "2025-05-04T18:26:49.910803Z",
     "iopub.status.idle": "2025-05-04T19:24:40.135176Z",
     "shell.execute_reply": "2025-05-04T19:24:40.134329Z"
    },
    "papermill": {
     "duration": 3470.235764,
     "end_time": "2025-05-04T19:24:40.136623",
     "exception": false,
     "start_time": "2025-05-04T18:26:49.900859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 [Train]:   0%|          | 0/125 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Epoch 1/12 [Train]: 100%|██████████| 125/125 [05:34<00:00,  2.68s/it, loss=1578.5988, lr=0.000100]\n",
      "Epoch 1/12 [Valid]: 100%|██████████| 14/14 [00:26<00:00,  1.92s/it, NME=4.7266]\n",
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New best model saved! NME: 4.2797\n",
      "Epoch 01 | Train Loss: 1497.9141 | Val NME: 4.2797 | Train Time: 334.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 [Train]: 100%|██████████| 125/125 [04:22<00:00,  2.10s/it, loss=1614.5026, lr=0.000100]\n",
      "Epoch 2/12 [Valid]: 100%|██████████| 14/14 [00:19<00:00,  1.40s/it, NME=4.7266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | Train Loss: 1497.9141 | Val NME: 4.2797 | Train Time: 262.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 [Train]: 100%|██████████| 125/125 [04:23<00:00,  2.11s/it, loss=1561.0507, lr=0.000100]\n",
      "Epoch 3/12 [Valid]: 100%|██████████| 14/14 [00:19<00:00,  1.42s/it, NME=4.7266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | Train Loss: 1497.9141 | Val NME: 4.2797 | Train Time: 263.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 [Train]: 100%|██████████| 125/125 [04:23<00:00,  2.11s/it, loss=1437.5396, lr=0.000100]\n",
      "Epoch 4/12 [Valid]: 100%|██████████| 14/14 [00:19<00:00,  1.42s/it, NME=4.7266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | Train Loss: 1497.9141 | Val NME: 4.2797 | Train Time: 263.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/12 [Train]: 100%|██████████| 125/125 [04:23<00:00,  2.11s/it, loss=1360.4073, lr=0.000100]\n",
      "Epoch 5/12 [Valid]: 100%|██████████| 14/14 [00:19<00:00,  1.42s/it, NME=4.7266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | Train Loss: 1497.9141 | Val NME: 4.2797 | Train Time: 263.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/12 [Train]: 100%|██████████| 125/125 [04:24<00:00,  2.11s/it, loss=1451.0034, lr=0.000100]\n",
      "Epoch 6/12 [Valid]: 100%|██████████| 14/14 [00:19<00:00,  1.42s/it, NME=4.7266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | Train Loss: 1497.9141 | Val NME: 4.2797 | Train Time: 264.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/12 [Train]: 100%|██████████| 125/125 [04:17<00:00,  2.06s/it, loss=1429.5950, lr=0.000010]\n",
      "Epoch 7/12 [Valid]: 100%|██████████| 14/14 [00:20<00:00,  1.45s/it, NME=4.7266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 | Train Loss: 1497.9141 | Val NME: 4.2797 | Train Time: 257.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/12 [Train]: 100%|██████████| 125/125 [04:26<00:00,  2.13s/it, loss=1448.7332, lr=0.000010]\n",
      "Epoch 8/12 [Valid]: 100%|██████████| 14/14 [00:20<00:00,  1.44s/it, NME=4.7266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | Train Loss: 1497.9141 | Val NME: 4.2797 | Train Time: 266.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/12 [Train]: 100%|██████████| 125/125 [04:24<00:00,  2.12s/it, loss=1279.7753, lr=0.000010]\n",
      "Epoch 9/12 [Valid]: 100%|██████████| 14/14 [00:19<00:00,  1.42s/it, NME=4.7266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 | Train Loss: 1497.9141 | Val NME: 4.2797 | Train Time: 264.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/12 [Train]: 100%|██████████| 125/125 [04:24<00:00,  2.12s/it, loss=1682.9082, lr=0.000010]\n",
      "Epoch 10/12 [Valid]: 100%|██████████| 14/14 [00:19<00:00,  1.41s/it, NME=4.7266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 1497.9141 | Val NME: 4.2797 | Train Time: 264.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/12 [Train]: 100%|██████████| 125/125 [04:16<00:00,  2.05s/it, loss=1648.5017, lr=0.000001]\n",
      "Epoch 11/12 [Valid]: 100%|██████████| 14/14 [00:19<00:00,  1.40s/it, NME=4.7266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 1497.9141 | Val NME: 4.2797 | Train Time: 256.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/12 [Train]: 100%|██████████| 125/125 [04:16<00:00,  2.05s/it, loss=1264.0491, lr=0.000001]\n",
      "Epoch 12/12 [Valid]: 100%|██████████| 14/14 [00:19<00:00,  1.41s/it, NME=4.7266]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 1497.9141 | Val NME: 4.2797 | Train Time: 256.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_nme = float('inf')\n",
    "best_model_wts = None\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\")\n",
    "    for batch in train_pbar:\n",
    "        imgs = batch['image'].to(device)\n",
    "        targets = batch['landmarks'].to(device, dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, dtype=torch.float16):\n",
    "            preds = model(imgs, labels=None, tasks=None)\n",
    "            loss = landmark_loss_fn(preds, targets)\n",
    "\n",
    "        # Scales loss. Calls backward() on scaled loss to create scaled gradients.\n",
    "        scaler.scale(loss).backward()\n",
    "        # Unscales gradients and calls optimizer.step() if gradients are not NaN\n",
    "        scaler.step(optimizer)\n",
    "        # Updates the scale for next iteration\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        train_pbar.set_postfix(\n",
    "            {'loss': f\"{loss.item():.4f}\", 'lr': f\"{optimizer.param_groups[0]['lr']:.6f}\"})\n",
    "\n",
    "    train_time = time.time() - epoch_start\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Free memory\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    nmes = []\n",
    "    val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\")\n",
    "    with torch.no_grad():\n",
    "        for batch in val_pbar:\n",
    "            imgs = batch['image'].to(device)\n",
    "            targets = batch['landmarks'].to(device, dtype=torch.float32)\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.float16):\n",
    "                preds = model(imgs, labels=None, tasks=None)\n",
    "                \n",
    "                # Convert heatmaps to coordinates\n",
    "                lms = heatmap_to_coordinates(preds)\n",
    "                # Denormalize coordinates\n",
    "                denorm_lms = denorm_points(lms, 224, 224)\n",
    "                \n",
    "                assert denorm_lms.shape == targets.shape, \\\n",
    "                    f\"Shape mismatch: {denorm_lms.shape} vs {targets.shape}\"\n",
    "                \n",
    "                # Calculate NME\n",
    "                batch_nmes = nme_metric.test(denorm_lms, targets)\n",
    "\n",
    "            nmes.extend(batch_nmes)\n",
    "            current_nme = sum(batch_nmes) / \\\n",
    "                len(batch_nmes) if batch_nmes else 0\n",
    "            val_pbar.set_postfix({'NME': f\"{current_nme:.4f}\"})\n",
    "\n",
    "    mean_nme = sum(nmes) / len(nmes)\n",
    "\n",
    "    # Free memory\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Scheduler step - this is correct as we're doing it once per epoch\n",
    "    # after all optimizer steps are completed\n",
    "    scheduler.step()\n",
    "\n",
    "    # Check for best\n",
    "    if mean_nme < best_nme:\n",
    "        # Free memory\n",
    "        del best_model_wts\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Save the best model\n",
    "        best_nme = mean_nme\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        print(f\"✅ New best model saved! NME: {mean_nme:.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} \"\n",
    "          f\"| Val NME: {mean_nme:.4f} | Train Time: {train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd19133f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T19:24:40.435987Z",
     "iopub.status.busy": "2025-05-04T19:24:40.435347Z",
     "iopub.status.idle": "2025-05-04T19:24:40.439155Z",
     "shell.execute_reply": "2025-05-04T19:24:40.438493Z"
    },
    "papermill": {
     "duration": 0.154351,
     "end_time": "2025-05-04T19:24:40.440498",
     "exception": false,
     "start_time": "2025-05-04T19:24:40.286147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [7] After training the model!!!!\n"
     ]
    }
   ],
   "source": [
    "# [LOG]\n",
    "print(\">> [7] After training the model!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d22e7",
   "metadata": {
    "papermill": {
     "duration": 0.148415,
     "end_time": "2025-05-04T19:24:40.735574",
     "exception": false,
     "start_time": "2025-05-04T19:24:40.587159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 8. Final Evaluation on Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "083319f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T19:24:41.079634Z",
     "iopub.status.busy": "2025-05-04T19:24:41.079359Z",
     "iopub.status.idle": "2025-05-04T19:25:23.369615Z",
     "shell.execute_reply": "2025-05-04T19:25:23.368673Z"
    },
    "papermill": {
     "duration": 42.434185,
     "end_time": "2025-05-04T19:25:23.370820",
     "exception": false,
     "start_time": "2025-05-04T19:24:40.936635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Testing]: 100%|██████████| 21/21 [00:42<00:00,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "  Mean NME    : 4.3846\n",
      "  FR (@0.05)  : 1.0000\n",
      "  AUC         : 0.000000\n",
      "  Inference/s : 437.0 img/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(best_model_wts)\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on test\n",
    "test_nmes = []\n",
    "inference_times = []\n",
    "with torch.no_grad():\n",
    "    test_pbar = tqdm(test_loader, desc=\"[Testing]\")\n",
    "    for batch in test_pbar:\n",
    "        imgs = batch['image'].to(device)\n",
    "        targets = batch['landmarks'].to(device, dtype=torch.float32)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, dtype=torch.float16):\n",
    "            start = time.time()\n",
    "            preds = model(imgs, labels=None, tasks=None)\n",
    "            end = time.time()\n",
    "\n",
    "            # Convert heatmaps to coordinates\n",
    "            lms = heatmap_to_coordinates(preds)\n",
    "            # Denormalize coordinates\n",
    "            denorm_lms = denorm_points(lms, 224, 224)\n",
    "\n",
    "            assert denorm_lms.shape == targets.shape, \\\n",
    "                f\"Shape mismatch: {denorm_lms.shape} vs {targets.shape}\"\n",
    "\n",
    "            # Calculate NME\n",
    "            test_nmes.extend(nme_metric.test(denorm_lms, targets))\n",
    "\n",
    "        inference_times.append((end - start) / imgs.size(0))\n",
    "\n",
    "# Metrics\n",
    "mean_test_nme = sum(test_nmes) / len(test_nmes)\n",
    "# [FR, AUC] using Simpson's rule :contentReference[oaicite:9]{index=9}\n",
    "fr, auc = fr_auc_metric.test(test_nmes)\n",
    "mean_inference_time = sum(inference_times) / len(inference_times)\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"  Mean NME    : {mean_test_nme:.4f}\")\n",
    "print(f\"  FR (@0.05)  : {fr:.4f}\")\n",
    "print(f\"  AUC         : {auc:.6f}\")\n",
    "print(f\"  Inference/s : {1/mean_inference_time:.1f} img/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13b200e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T19:25:23.662929Z",
     "iopub.status.busy": "2025-05-04T19:25:23.662257Z",
     "iopub.status.idle": "2025-05-04T19:25:23.666193Z",
     "shell.execute_reply": "2025-05-04T19:25:23.665636Z"
    },
    "papermill": {
     "duration": 0.147064,
     "end_time": "2025-05-04T19:25:23.667272",
     "exception": false,
     "start_time": "2025-05-04T19:25:23.520208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [8] After evaluating the model on test set!!!!\n"
     ]
    }
   ],
   "source": [
    "# [LOG]\n",
    "print(\">> [8] After evaluating the model on test set!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dbcd47",
   "metadata": {
    "papermill": {
     "duration": 0.14681,
     "end_time": "2025-05-04T19:25:23.955382",
     "exception": false,
     "start_time": "2025-05-04T19:25:23.808572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 9. Save best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e99a6e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T19:25:24.249400Z",
     "iopub.status.busy": "2025-05-04T19:25:24.249081Z",
     "iopub.status.idle": "2025-05-04T19:25:24.752903Z",
     "shell.execute_reply": "2025-05-04T19:25:24.752141Z"
    },
    "papermill": {
     "duration": 0.65664,
     "end_time": "2025-05-04T19:25:24.754032",
     "exception": false,
     "start_time": "2025-05-04T19:25:24.097392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to ./models/facexformer_swin_b_best.pth\n",
      "Model metrics - NME: 4.3846, FR@0.05: 1.0000, AUC: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Create directory for saving models if it doesn't exist\n",
    "save_dir = './models'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save the best model weights\n",
    "model_save_path = os.path.join(\n",
    "    save_dir, f'facexformer_{backbone_name}_best.pth')\n",
    "torch.save({\n",
    "    # Save backbone name\n",
    "    'backbone_name': backbone_name,\n",
    "    # Save model state dict\n",
    "    'state_dict': best_model_wts,\n",
    "    # Save metric values on test set\n",
    "    'metrics': {\n",
    "        'nme': mean_test_nme,\n",
    "        'fr': fr,\n",
    "        'auc': auc,\n",
    "        'inference_time': mean_inference_time\n",
    "    },\n",
    "}, model_save_path)\n",
    "\n",
    "print(f\"Best model saved to {model_save_path}\")\n",
    "print(\n",
    "    f\"Model metrics - NME: {mean_test_nme:.4f}, FR@0.05: {fr:.4f}, AUC: {auc:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121aabf",
   "metadata": {
    "papermill": {
     "duration": 0.195776,
     "end_time": "2025-05-04T19:25:25.097488",
     "exception": false,
     "start_time": "2025-05-04T19:25:24.901712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 10. Load best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3709a3a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T19:25:25.391162Z",
     "iopub.status.busy": "2025-05-04T19:25:25.390652Z",
     "iopub.status.idle": "2025-05-04T19:25:28.064677Z",
     "shell.execute_reply": "2025-05-04T19:25:28.063852Z"
    },
    "papermill": {
     "duration": 2.822006,
     "end_time": "2025-05-04T19:25:28.065869",
     "exception": false,
     "start_time": "2025-05-04T19:25:25.243863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FaceXFormer model with swin_b backbone\n",
      "Metrics on test set:\n",
      "  Mean NME    : 4.3846\n",
      "  FR (@0.05)  : 1.0000\n",
      "  AUC         : 0.000000\n",
      "  Inference/s : 437.0 img/s\n"
     ]
    }
   ],
   "source": [
    "# Load the best model with exact backbone\n",
    "def load_facexformer_model(model_path):\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Model file not found at {model_path}\")\n",
    "        return None, None\n",
    "\n",
    "    # Load model checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=device,\n",
    "                            weights_only=False)\n",
    "    backbone_name = checkpoint['backbone_name']\n",
    "\n",
    "    # Initialize model with the same backbone\n",
    "    model = FaceXFormer(backbone_name=backbone_name)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Get metrics\n",
    "    metrics = checkpoint['metrics']\n",
    "\n",
    "    print(f\"Loaded FaceXFormer model with {backbone_name} backbone\")\n",
    "    print(f\"Metrics on test set:\")\n",
    "    print(f\"  Mean NME    : {metrics['nme']:.4f}\")\n",
    "    print(f\"  FR (@0.05)  : {metrics['fr']:.4f}\")\n",
    "    print(f\"  AUC         : {metrics['auc']:.6f}\")\n",
    "    print(f\"  Inference/s : {1/metrics['inference_time']:.1f} img/s\")\n",
    "\n",
    "    return model, metrics\n",
    "\n",
    "\n",
    "# Load the best model\n",
    "model_path = os.path.join(save_dir, f'facexformer_{backbone_name}_best.pth')\n",
    "loaded_model, loaded_metrics = load_facexformer_model(model_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2029045,
     "sourceId": 3364057,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3560.891,
   "end_time": "2025-05-04T19:25:31.845682",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-04T18:26:10.954682",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
