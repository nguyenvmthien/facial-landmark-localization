{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f03ed51",
   "metadata": {},
   "source": [
    "# 1. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d5a57b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:27.871599Z",
     "iopub.status.busy": "2025-05-04T16:17:27.871136Z",
     "iopub.status.idle": "2025-05-04T16:17:31.013610Z",
     "shell.execute_reply": "2025-05-04T16:17:31.012838Z",
     "shell.execute_reply.started": "2025-05-04T16:17:27.871573Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from typing import Any, Optional, Tuple, Type\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import copy\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.integrate import simpson as simps\n",
    "import math\n",
    "from torchvision.models import swin_b\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "738ae701",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.014957Z",
     "iopub.status.busy": "2025-05-04T16:17:31.014509Z",
     "iopub.status.idle": "2025-05-04T16:17:31.019527Z",
     "shell.execute_reply": "2025-05-04T16:17:31.018862Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.014930Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [1] After importing libraries!!!!\n"
     ]
    }
   ],
   "source": [
    "# [LOG]\n",
    "print(\">> [1] After importing libraries!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcbd66b",
   "metadata": {},
   "source": [
    "# 2. Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aac2599",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.020531Z",
     "iopub.status.busy": "2025-05-04T16:17:31.020300Z",
     "iopub.status.idle": "2025-05-04T16:17:31.033322Z",
     "shell.execute_reply": "2025-05-04T16:17:31.032695Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.020507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    \"\"\" MLP Block for Transformer \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embedding_dim: int,\n",
    "                 mlp_dim: int,\n",
    "                 activation: Type[nn.Module] = nn.GELU) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(embedding_dim, mlp_dim)\n",
    "        self.linear_2 = nn.Linear(mlp_dim, embedding_dim)\n",
    "        self.activation = activation()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.linear_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf42dbbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.035995Z",
     "iopub.status.busy": "2025-05-04T16:17:31.035797Z",
     "iopub.status.idle": "2025-05-04T16:17:31.052276Z",
     "shell.execute_reply": "2025-05-04T16:17:31.051694Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.035980Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# From https://github.com/facebookresearch/detectron2/blob/main/detectron2/layers/batch_norm.py # noqa\n",
    "# Itself from https://github.com/facebookresearch/ConvNeXt/blob/d1fa8f6fef0a165b27399986cc2bdacc92777e40/models/convnext.py#L119  # noqa\n",
    "class LayerNorm2d(nn.Module):\n",
    "\n",
    "    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d942d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.053517Z",
     "iopub.status.busy": "2025-05-04T16:17:31.053024Z",
     "iopub.status.idle": "2025-05-04T16:17:31.065453Z",
     "shell.execute_reply": "2025-05-04T16:17:31.064891Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.053493Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    An attention layer that allows for downscaling the size of the embedding\n",
    "    after projection to queries, keys, and values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        downsample_rate: int = 1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.internal_dim = embedding_dim // downsample_rate\n",
    "        self.num_heads = num_heads\n",
    "        assert self.internal_dim % num_heads == 0, \"num_heads must divide embedding_dim.\"\n",
    "\n",
    "        self.q_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "        self.k_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "        self.v_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "        self.out_proj = nn.Linear(self.internal_dim, embedding_dim)\n",
    "\n",
    "    def _separate_heads(self, x: Tensor, num_heads: int) -> Tensor:\n",
    "        b, n, c = x.shape\n",
    "        x = x.reshape(b, n, num_heads, c // num_heads)\n",
    "        return x.transpose(1, 2)  # B x N_heads x N_tokens x C_per_head\n",
    "\n",
    "    def _recombine_heads(self, x: Tensor) -> Tensor:\n",
    "        b, n_heads, n_tokens, c_per_head = x.shape\n",
    "        x = x.transpose(1, 2)\n",
    "        return x.reshape(b, n_tokens, n_heads * c_per_head)  # B x N_tokens x C\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
    "        # Input projections\n",
    "        q = self.q_proj(q)\n",
    "        k = self.k_proj(k)\n",
    "        v = self.v_proj(v)\n",
    "\n",
    "        # Separate into heads\n",
    "        q = self._separate_heads(q, self.num_heads)\n",
    "        k = self._separate_heads(k, self.num_heads)\n",
    "        v = self._separate_heads(v, self.num_heads)\n",
    "\n",
    "        # Attention\n",
    "        _, _, _, c_per_head = q.shape\n",
    "        attn = q @ k.permute(0, 1, 3, 2)  # B x N_heads x N_tokens x N_tokens\n",
    "        attn = attn / math.sqrt(c_per_head)\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "\n",
    "        # Get output\n",
    "        out = attn @ v\n",
    "        out = self._recombine_heads(out)\n",
    "        out = self.out_proj(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbae774b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.066308Z",
     "iopub.status.busy": "2025-05-04T16:17:31.066139Z",
     "iopub.status.idle": "2025-05-04T16:17:31.081182Z",
     "shell.execute_reply": "2025-05-04T16:17:31.080532Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.066294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TwoWayAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_dim: int = 2048,\n",
    "        activation: Type[nn.Module] = nn.ReLU,\n",
    "        attention_downsample_rate: int = 2,\n",
    "        skip_first_layer_pe: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        A transformer block with four layers: (1) self-attention of sparse\n",
    "        inputs, (2) cross attention of sparse inputs to dense inputs, (3) mlp\n",
    "        block on sparse inputs, and (4) cross attention of dense inputs to sparse\n",
    "        inputs.\n",
    "\n",
    "        Arguments:\n",
    "          embedding_dim (int): the channel dimension of the embeddings\n",
    "          num_heads (int): the number of heads in the attention layers\n",
    "          mlp_dim (int): the hidden dimension of the mlp block\n",
    "          activation (nn.Module): the activation of the mlp block\n",
    "          skip_first_layer_pe (bool): skip the PE on the first layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attn = Attention(embedding_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.cross_attn_token_to_image = Attention(\n",
    "            embedding_dim, num_heads,\n",
    "            downsample_rate=attention_downsample_rate\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.mlp = MLPBlock(embedding_dim, mlp_dim, activation)\n",
    "        self.norm3 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.cross_attn_image_to_token = Attention(\n",
    "            embedding_dim, num_heads,\n",
    "            downsample_rate=attention_downsample_rate\n",
    "        )\n",
    "        self.norm4 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.skip_first_layer_pe = skip_first_layer_pe\n",
    "\n",
    "    def forward(\n",
    "        self, queries: Tensor, keys: Tensor, query_pe: Tensor, key_pe: Tensor\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        # Self attention block\n",
    "        if self.skip_first_layer_pe:\n",
    "            queries = self.self_attn(q=queries, k=queries, v=queries)\n",
    "        else:\n",
    "            q = queries + query_pe\n",
    "            attn_out = self.self_attn(q=q, k=q, v=queries)\n",
    "            queries = queries + attn_out\n",
    "        queries = self.norm1(queries)\n",
    "\n",
    "        # Cross attention block, tokens attending to image embedding\n",
    "        q = queries + query_pe\n",
    "        k = keys + key_pe\n",
    "        attn_out = self.cross_attn_token_to_image(q=q, k=k, v=keys)\n",
    "        queries = queries + attn_out\n",
    "        queries = self.norm2(queries)\n",
    "\n",
    "        # MLP block\n",
    "        mlp_out = self.mlp(queries)\n",
    "        queries = queries + mlp_out\n",
    "        queries = self.norm3(queries)\n",
    "\n",
    "        # Cross attention block, image embedding attending to tokens\n",
    "        q = queries + query_pe\n",
    "        k = keys + key_pe\n",
    "        attn_out = self.cross_attn_image_to_token(q=k, k=q, v=queries)\n",
    "        keys = keys + attn_out\n",
    "        keys = self.norm4(keys)\n",
    "\n",
    "        return queries, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "960a42fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.081952Z",
     "iopub.status.busy": "2025-05-04T16:17:31.081789Z",
     "iopub.status.idle": "2025-05-04T16:17:31.095365Z",
     "shell.execute_reply": "2025-05-04T16:17:31.094832Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.081938Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TwoWayTransformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        depth: int,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_dim: int,\n",
    "        activation: Type[nn.Module] = nn.ReLU,\n",
    "        attention_downsample_rate: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        A transformer decoder that attends to an input image using\n",
    "        queries whose positional embedding is supplied.\n",
    "\n",
    "        Args:\n",
    "          depth (int): number of layers in the transformer\n",
    "          embedding_dim (int): the channel dimension for the input embeddings\n",
    "          num_heads (int): the number of heads for multihead attention. Must\n",
    "            divide embedding_dim\n",
    "          mlp_dim (int): the channel dimension internal to the MLP block\n",
    "          activation (nn.Module): the activation to use in the MLP block\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(depth):\n",
    "            self.layers.append(\n",
    "                TwoWayAttentionBlock(\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    mlp_dim=mlp_dim,\n",
    "                    activation=activation,\n",
    "                    attention_downsample_rate=attention_downsample_rate,\n",
    "                    skip_first_layer_pe=(i == 0),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.final_attn_token_to_image = Attention(\n",
    "            embedding_dim, num_heads,\n",
    "            downsample_rate=attention_downsample_rate\n",
    "        )\n",
    "        self.norm_final_attn = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        image_embedding: Tensor,\n",
    "        image_pe: Tensor,\n",
    "        point_embedding: Tensor,\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          image_embedding (torch.Tensor): image to attend to. Should be shape\n",
    "            B x embedding_dim x h x w for any h and w.\n",
    "          image_pe (torch.Tensor): the positional encoding to add to the image. Must\n",
    "            have the same shape as image_embedding.\n",
    "          point_embedding (torch.Tensor): the embedding to add to the query points.\n",
    "            Must have shape B x N_points x embedding_dim for any N_points.\n",
    "\n",
    "        Returns:\n",
    "          torch.Tensor: the processed point_embedding\n",
    "          torch.Tensor: the processed image_embedding\n",
    "        \"\"\"\n",
    "        # BxCxHxW -> BxHWxC == B x N_image_tokens x C\n",
    "        bs, c, h, w = image_embedding.shape\n",
    "        image_embedding = image_embedding.flatten(2).permute(0, 2, 1)\n",
    "        image_pe = image_pe.flatten(2).permute(0, 2, 1)\n",
    "\n",
    "        # Prepare queries\n",
    "        queries = point_embedding\n",
    "        keys = image_embedding\n",
    "\n",
    "        # Apply transformer blocks and final layernorm\n",
    "        for layer in self.layers:\n",
    "            queries, keys = layer(\n",
    "                queries=queries,\n",
    "                keys=keys,\n",
    "                query_pe=point_embedding,\n",
    "                key_pe=image_pe,\n",
    "            )\n",
    "\n",
    "        # Apply the final attention layer from the points to the image\n",
    "        q = queries + point_embedding\n",
    "        k = keys + image_pe\n",
    "        attn_out = self.final_attn_token_to_image(q=q, k=k, v=keys)\n",
    "        queries = queries + attn_out\n",
    "        queries = self.norm_final_attn(queries)\n",
    "\n",
    "        return queries, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7767f318",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.096134Z",
     "iopub.status.busy": "2025-05-04T16:17:31.095965Z",
     "iopub.status.idle": "2025-05-04T16:17:31.110275Z",
     "shell.execute_reply": "2025-05-04T16:17:31.109734Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.096119Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [2] After defining Transformer classes!!!!\n"
     ]
    }
   ],
   "source": [
    "# [LOG]\n",
    "print(\">> [2] After defining Transformer classes!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feee7e4",
   "metadata": {},
   "source": [
    "# 3. FaceXFormer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f350ef8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.111115Z",
     "iopub.status.busy": "2025-05-04T16:17:31.110924Z",
     "iopub.status.idle": "2025-05-04T16:17:31.122046Z",
     "shell.execute_reply": "2025-05-04T16:17:31.121482Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.111100Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        output_dim: int,\n",
    "        num_layers: int,\n",
    "        sigmoid_output: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(\n",
    "            nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim])\n",
    "        )\n",
    "        self.sigmoid_output = sigmoid_output\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        if self.sigmoid_output:\n",
    "            x = F.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "512d9d95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.122905Z",
     "iopub.status.busy": "2025-05-04T16:17:31.122692Z",
     "iopub.status.idle": "2025-05-04T16:17:31.134165Z",
     "shell.execute_reply": "2025-05-04T16:17:31.133633Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.122891Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FaceDecoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        transformer_dim: 256,\n",
    "        transformer: nn.Module,\n",
    "        activation: Type[nn.Module] = nn.GELU,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.transformer_dim = transformer_dim\n",
    "        self.transformer = transformer\n",
    "\n",
    "        self.landmarks_token = nn.Embedding(1, transformer_dim)\n",
    "\n",
    "        self.landmarks_prediction_head = MLP(\n",
    "            transformer_dim, transformer_dim, 136, 3\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        image_embeddings: Tensor,\n",
    "        image_pe: Tensor,\n",
    "    ) -> Tensor:\n",
    "        output_tokens = torch.cat([self.landmarks_token.weight], dim=0)\n",
    "        tokens = output_tokens.unsqueeze(0).expand(\n",
    "            image_embeddings.size(0), -1, -1)\n",
    "\n",
    "        src = image_embeddings\n",
    "        pos_src = image_pe.expand(image_embeddings.size(0), -1, -1, -1)\n",
    "        b, c, h, w = src.shape\n",
    "\n",
    "        hs, src = self.transformer(src, pos_src, tokens)\n",
    "\n",
    "        landmarks_token_out = hs[:, 0, :]\n",
    "\n",
    "        landmark_output = self.landmarks_prediction_head(landmarks_token_out)\n",
    "\n",
    "        return landmark_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93898ac8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.135462Z",
     "iopub.status.busy": "2025-05-04T16:17:31.134984Z",
     "iopub.status.idle": "2025-05-04T16:17:31.150281Z",
     "shell.execute_reply": "2025-05-04T16:17:31.149784Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.135438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionEmbeddingRandom(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding using random spatial frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_pos_feats: int = 64, scale: Optional[float] = None) -> None:\n",
    "        super().__init__()\n",
    "        if scale is None or scale <= 0.0:\n",
    "            scale = 1.0\n",
    "        self.register_buffer(\n",
    "            \"positional_encoding_gaussian_matrix\",\n",
    "            scale * torch.randn((2, num_pos_feats)),\n",
    "        )\n",
    "\n",
    "    def _pe_encoding(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n",
    "        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n",
    "        coords = 2 * coords - 1\n",
    "        coords = coords @ self.positional_encoding_gaussian_matrix\n",
    "        coords = 2 * np.pi * coords\n",
    "        # outputs d_1 x ... x d_n x C shape\n",
    "        return torch.cat([torch.sin(coords), torch.cos(coords)], dim=-1)\n",
    "\n",
    "    def forward(self, size: Tuple[int, int]) -> torch.Tensor:\n",
    "        \"\"\"Generate positional encoding for a grid of the specified size.\"\"\"\n",
    "        h, w = size\n",
    "        device: Any = self.positional_encoding_gaussian_matrix.device\n",
    "        grid = torch.ones((h, w), device=device, dtype=torch.float32)\n",
    "        y_embed = grid.cumsum(dim=0) - 0.5\n",
    "        x_embed = grid.cumsum(dim=1) - 0.5\n",
    "        y_embed = y_embed / h\n",
    "        x_embed = x_embed / w\n",
    "\n",
    "        pe = self._pe_encoding(torch.stack([x_embed, y_embed], dim=-1))\n",
    "        return pe.permute(2, 0, 1)  # C x H x W\n",
    "\n",
    "    def forward_with_coords(\n",
    "        self, coords_input: torch.Tensor, image_size: Tuple[int, int]\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Positionally encode points that are not normalized to [0,1].\"\"\"\n",
    "        coords = coords_input.clone()\n",
    "        coords[:, :, 0] = coords[:, :, 0] / image_size[1]\n",
    "        coords[:, :, 1] = coords[:, :, 1] / image_size[0]\n",
    "        return self._pe_encoding(coords.to(torch.float))  # B x N x C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a0fc07b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.151411Z",
     "iopub.status.busy": "2025-05-04T16:17:31.150990Z",
     "iopub.status.idle": "2025-05-04T16:17:31.165258Z",
     "shell.execute_reply": "2025-05-04T16:17:31.164566Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.151394Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FaceXFormerMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(input_dim, 256)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        hidden_states = hidden_states.flatten(2).transpose(1, 2)\n",
    "        hidden_states = self.proj(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7612844d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.168347Z",
     "iopub.status.busy": "2025-05-04T16:17:31.167915Z",
     "iopub.status.idle": "2025-05-04T16:17:31.188969Z",
     "shell.execute_reply": "2025-05-04T16:17:31.188295Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.168330Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FaceXFormer(nn.Module):\n",
    "\n",
    "    def __init__(self, backbone_name: str):\n",
    "        super(FaceXFormer, self).__init__()\n",
    "\n",
    "        if backbone_name != 'swin_b':\n",
    "            raise ValueError(f\"Unsupported backbone name: {backbone_name}\")\n",
    "\n",
    "        swin_v2 = swin_b(weights='IMAGENET1K_V1')\n",
    "        self.backbone = torch.nn.Sequential(*(list(swin_v2.children())[:-1]))\n",
    "        self.target_layer_names = ['0.1', '0.3', '0.5', '0.7']\n",
    "        self.multi_scale_features = []\n",
    "\n",
    "        embed_dim = 1024\n",
    "        out_chans = 256\n",
    "\n",
    "        self.pe_layer = PositionEmbeddingRandom(out_chans // 2)\n",
    "\n",
    "        for name, module in self.backbone.named_modules():\n",
    "            if name in self.target_layer_names:\n",
    "                module.register_forward_hook(self.save_features_hook(name))\n",
    "\n",
    "        self.face_decoder = FaceDecoder(\n",
    "            transformer_dim=256,\n",
    "            transformer=TwoWayTransformer(\n",
    "                depth=2,\n",
    "                embedding_dim=256,\n",
    "                mlp_dim=2048,\n",
    "                num_heads=8,\n",
    "            ))\n",
    "\n",
    "        num_encoder_blocks = 4\n",
    "        hidden_sizes = [128, 256, 512, 1024]\n",
    "        decoder_hidden_size = 256\n",
    "\n",
    "        mlps = []\n",
    "        for i in range(num_encoder_blocks):\n",
    "            mlp = FaceXFormerMLP(input_dim=hidden_sizes[i])\n",
    "            mlps.append(mlp)\n",
    "        self.linear_c = nn.ModuleList(mlps)\n",
    "\n",
    "        self.linear_fuse = nn.Conv2d(\n",
    "            in_channels=decoder_hidden_size * num_encoder_blocks,\n",
    "            out_channels=decoder_hidden_size,\n",
    "            kernel_size=1,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "    def save_features_hook(self, name):\n",
    "        def hook(module, input, output):\n",
    "            self.multi_scale_features.append(\n",
    "                output.permute(0, 3, 1, 2).contiguous())\n",
    "        return hook\n",
    "\n",
    "    def forward(self, x, labels, tasks):\n",
    "        self.multi_scale_features.clear()\n",
    "\n",
    "        _, _, h, w = x.shape\n",
    "        features = self.backbone(x).squeeze()\n",
    "\n",
    "        batch_size = self.multi_scale_features[-1].shape[0]\n",
    "        all_hidden_states = ()\n",
    "        for encoder_hidden_state, mlp in zip(self.multi_scale_features, self.linear_c):\n",
    "\n",
    "            height, width = encoder_hidden_state.shape[2], encoder_hidden_state.shape[3]\n",
    "            encoder_hidden_state = mlp(encoder_hidden_state)\n",
    "            encoder_hidden_state = encoder_hidden_state.permute(0, 2, 1)\n",
    "            encoder_hidden_state = encoder_hidden_state.reshape(\n",
    "                batch_size, -1, height, width)\n",
    "            encoder_hidden_state = nn.functional.interpolate(\n",
    "                encoder_hidden_state, size=self.multi_scale_features[0].size()[\n",
    "                    2:],\n",
    "                mode=\"bilinear\", align_corners=False\n",
    "            )\n",
    "            all_hidden_states += (encoder_hidden_state,)\n",
    "\n",
    "        fused_states = self.linear_fuse(\n",
    "            torch.cat(all_hidden_states[::-1], dim=1))\n",
    "        image_pe = self.pe_layer(\n",
    "            (fused_states.shape[2], fused_states.shape[3])).unsqueeze(0)\n",
    "\n",
    "        landmark_output = self.face_decoder(\n",
    "            image_embeddings=fused_states,\n",
    "            image_pe=image_pe\n",
    "        )\n",
    "\n",
    "        return landmark_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2386999",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.190049Z",
     "iopub.status.busy": "2025-05-04T16:17:31.189740Z",
     "iopub.status.idle": "2025-05-04T16:17:31.205653Z",
     "shell.execute_reply": "2025-05-04T16:17:31.204895Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.190032Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [3] After defining FaceXFormer model!!!!\n"
     ]
    }
   ],
   "source": [
    "# [LOG]\n",
    "print(\">> [3] After defining FaceXFormer model!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e4f6a2",
   "metadata": {},
   "source": [
    "# 4. Loss functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b1a6bc",
   "metadata": {},
   "source": [
    "## 4.1. WingLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38a50cd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.206615Z",
     "iopub.status.busy": "2025-05-04T16:17:31.206432Z",
     "iopub.status.idle": "2025-05-04T16:17:31.217405Z",
     "shell.execute_reply": "2025-05-04T16:17:31.216653Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.206593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Wing Loss\n",
    "class WingLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, omega=0.01, epsilon=2):\n",
    "        super(WingLoss, self).__init__()\n",
    "        self.omega = omega\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        y = target\n",
    "        y_hat = pred\n",
    "        delta_2 = (y - y_hat).pow(2).sum(dim=-1, keepdim=False)\n",
    "        # delta = delta_2.sqrt()\n",
    "        delta = delta_2.clamp(min=1e-6).sqrt()\n",
    "        C = self.omega - self.omega * math.log(1 + self.omega / self.epsilon)\n",
    "        loss = torch.where(\n",
    "            delta < self.omega,\n",
    "            self.omega * torch.log(1 + delta / self.epsilon),\n",
    "            delta - C\n",
    "        )\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1d5f3c",
   "metadata": {},
   "source": [
    "## 4.2. SmoothL1Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f596b937",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.218789Z",
     "iopub.status.busy": "2025-05-04T16:17:31.218253Z",
     "iopub.status.idle": "2025-05-04T16:17:31.233279Z",
     "shell.execute_reply": "2025-05-04T16:17:31.232706Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.218766Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Smooth L1 Loss\n",
    "class SmoothL1Loss(nn.Module):\n",
    "\n",
    "    def __init__(self, scale=0.01):\n",
    "        super(SmoothL1Loss, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.EPSILON = 1e-10\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"SmoothL1Loss()\"\n",
    "\n",
    "    def forward(self, output: torch.Tensor, groundtruth: torch.Tensor, reduction='mean'):\n",
    "        \"\"\"\n",
    "            input:  b x n x 2\n",
    "            output: b x n x 1 => 1\n",
    "        \"\"\"\n",
    "        if output.dim() == 4:\n",
    "            shape = output.shape\n",
    "            groundtruth = groundtruth.reshape(shape[0], shape[1], 1, shape[3])\n",
    "\n",
    "        delta_2 = (output - groundtruth).pow(2).sum(dim=-1, keepdim=False)\n",
    "        delta = delta_2.clamp(min=1e-6).sqrt()\n",
    "        # delta = torch.sqrt(delta_2 + self.EPSILON)\n",
    "        loss = torch.where(\n",
    "            delta_2 < self.scale * self.scale,\n",
    "            0.5 / self.scale * delta_2,\n",
    "            delta - 0.5 * self.scale)\n",
    "\n",
    "        if reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        elif reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079644db",
   "metadata": {},
   "source": [
    "## 4.3. AWingLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f02869c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.234160Z",
     "iopub.status.busy": "2025-05-04T16:17:31.233922Z",
     "iopub.status.idle": "2025-05-04T16:17:31.247902Z",
     "shell.execute_reply": "2025-05-04T16:17:31.247193Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.234141Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# AWingLoss\n",
    "class AWingLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, omega=14, theta=0.5, epsilon=1, alpha=2.1, use_weight_map=True):\n",
    "        super(AWingLoss, self).__init__()\n",
    "        self.omega = omega\n",
    "        self.theta = theta\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.use_weight_map = use_weight_map\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"AWingLoss()\"\n",
    "\n",
    "    def generate_weight_map(self, heatmap, k_size=3, w=10):\n",
    "        dilate = F.max_pool2d(heatmap, kernel_size=k_size, stride=1, padding=1)\n",
    "        weight_map = torch.where(dilate < 0.2, torch.zeros_like(\n",
    "            heatmap), torch.ones_like(heatmap))\n",
    "        return w * weight_map + 1\n",
    "\n",
    "    def forward(self, output, groundtruth):\n",
    "        \"\"\"\n",
    "        input:  b x n x h x w\n",
    "        output: b x n x h x w => 1\n",
    "        \"\"\"\n",
    "        delta = (output - groundtruth).abs()\n",
    "        A = self.omega * (1 / (1 + torch.pow(self.theta / self.epsilon, self.alpha - groundtruth))) * (self.alpha - groundtruth) * \\\n",
    "            (torch.pow(self.theta / self.epsilon, self.alpha -\n",
    "             groundtruth - 1)) * (1 / self.epsilon)\n",
    "        C = self.theta * A - self.omega * \\\n",
    "            torch.log(1 + torch.pow(self.theta /\n",
    "                      self.epsilon, self.alpha - groundtruth))\n",
    "        loss = torch.where(delta < self.theta,\n",
    "                           self.omega *\n",
    "                           torch.log(\n",
    "                               1 + torch.pow(delta / self.epsilon, self.alpha - groundtruth)),\n",
    "                           (A * delta - C))\n",
    "        if self.use_weight_map:\n",
    "            weight = self.generate_weight_map(groundtruth)\n",
    "            loss = loss * weight\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edbb2ab",
   "metadata": {},
   "source": [
    "## 4.4. STARLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2570af24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.249123Z",
     "iopub.status.busy": "2025-05-04T16:17:31.248892Z",
     "iopub.status.idle": "2025-05-04T16:17:31.266101Z",
     "shell.execute_reply": "2025-05-04T16:17:31.265459Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.249102Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# STAR Loss\n",
    "def get_channel_sum(input):\n",
    "    temp = torch.sum(input, dim=3)\n",
    "    output = torch.sum(temp, dim=2)\n",
    "    return output\n",
    "\n",
    "\n",
    "def expand_two_dimensions_at_end(input, dim1, dim2):\n",
    "    input = input.unsqueeze(-1).unsqueeze(-1)\n",
    "    input = input.expand(-1, -1, dim1, dim2)\n",
    "    return input\n",
    "\n",
    "\n",
    "class STARLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, w=1, dist='smoothl1', num_dim_image=2, EPSILON=1e-5):\n",
    "        super(STARLoss, self).__init__()\n",
    "        self.w = w\n",
    "        self.num_dim_image = num_dim_image\n",
    "        self.EPSILON = EPSILON\n",
    "        self.dist = dist\n",
    "        if self.dist == 'smoothl1':\n",
    "            self.dist_func = SmoothL1Loss()\n",
    "        elif self.dist == 'l1':\n",
    "            self.dist_func = F.l1_loss\n",
    "        elif self.dist == 'l2':\n",
    "            self.dist_func = F.mse_loss\n",
    "        elif self.dist == 'wing':\n",
    "            self.dist_func = WingLoss()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"STARLoss()\"\n",
    "\n",
    "    def _make_grid(self, h, w):\n",
    "        yy, xx = torch.meshgrid(\n",
    "            torch.arange(h).float() / (h - 1) * 2 - 1,\n",
    "            torch.arange(w).float() / (w - 1) * 2 - 1)\n",
    "        return yy, xx\n",
    "\n",
    "    def weighted_mean(self, heatmap):\n",
    "        batch, npoints, h, w = heatmap.shape\n",
    "\n",
    "        yy, xx = self._make_grid(h, w)\n",
    "        yy = yy.view(1, 1, h, w).to(heatmap)\n",
    "        xx = xx.view(1, 1, h, w).to(heatmap)\n",
    "\n",
    "        yy_coord = (yy * heatmap).sum([2, 3])  # batch x npoints\n",
    "        xx_coord = (xx * heatmap).sum([2, 3])  # batch x npoints\n",
    "        coords = torch.stack([xx_coord, yy_coord], dim=-1)\n",
    "        return coords\n",
    "\n",
    "    def unbiased_weighted_covariance(self, htp, means, num_dim_image=2, EPSILON=1e-5):\n",
    "        batch_size, num_points, height, width = htp.shape\n",
    "\n",
    "        yv, xv = self._make_grid(height, width)\n",
    "        xv = Variable(xv)\n",
    "        yv = Variable(yv)\n",
    "\n",
    "        if htp.is_cuda:\n",
    "            xv = xv.cuda()\n",
    "            yv = yv.cuda()\n",
    "\n",
    "        xmean = means[:, :, 0]\n",
    "        xv_minus_mean = xv.expand(batch_size, num_points, -1, -1) - expand_two_dimensions_at_end(xmean, height,\n",
    "                                                                                                 width)  # [batch_size, 68, 64, 64]\n",
    "        ymean = means[:, :, 1]\n",
    "        yv_minus_mean = yv.expand(batch_size, num_points, -1, -1) - expand_two_dimensions_at_end(ymean, height,\n",
    "                                                                                                 width)  # [batch_size, 68, 64, 64]\n",
    "        wt_xv_minus_mean = xv_minus_mean\n",
    "        wt_yv_minus_mean = yv_minus_mean\n",
    "\n",
    "        wt_xv_minus_mean = wt_xv_minus_mean.view(\n",
    "            batch_size * num_points, height * width)  # [batch_size*68, 4096]\n",
    "        wt_xv_minus_mean = wt_xv_minus_mean.view(\n",
    "            batch_size * num_points, 1, height * width)  # [batch_size*68, 1, 4096]\n",
    "        wt_yv_minus_mean = wt_yv_minus_mean.view(\n",
    "            batch_size * num_points, height * width)  # [batch_size*68, 4096]\n",
    "        wt_yv_minus_mean = wt_yv_minus_mean.view(\n",
    "            batch_size * num_points, 1, height * width)  # [batch_size*68, 1, 4096]\n",
    "        # [batch_size*68, 2, 4096]\n",
    "        vec_concat = torch.cat((wt_xv_minus_mean, wt_yv_minus_mean), 1)\n",
    "\n",
    "        htp_vec = htp.view(batch_size * num_points, 1, height * width)\n",
    "        htp_vec = htp_vec.expand(-1, 2, -1)\n",
    "\n",
    "        # [batch_size*68, 2, 2]\n",
    "        covariance = torch.bmm(htp_vec * vec_concat,\n",
    "                               vec_concat.transpose(1, 2))\n",
    "        covariance = covariance.view(\n",
    "            batch_size, num_points, num_dim_image, num_dim_image)  # [batch_size, 68, 2, 2]\n",
    "\n",
    "        V_1 = htp.sum([2, 3]) + EPSILON  # [batch_size, 68]\n",
    "        V_2 = torch.pow(htp, 2).sum([2, 3]) + EPSILON  # [batch_size, 68]\n",
    "\n",
    "        denominator = V_1 - (V_2 / V_1)\n",
    "        covariance = covariance / \\\n",
    "            expand_two_dimensions_at_end(\n",
    "                denominator, num_dim_image, num_dim_image)\n",
    "\n",
    "        return covariance\n",
    "\n",
    "    def ambiguity_guided_decompose(self, pts, eigenvalues, eigenvectors):\n",
    "        batch_size, npoints = pts.shape[:2]\n",
    "        rotate = torch.matmul(\n",
    "            pts.view(batch_size, npoints, 1, 2), eigenvectors.transpose(-1, -2))\n",
    "        scale = rotate.view(batch_size, npoints, 2) / \\\n",
    "            torch.sqrt(eigenvalues + self.EPSILON)\n",
    "        return scale\n",
    "\n",
    "    def eigenvalue_restriction(self, evalues, batch, npoints):\n",
    "        eigen_loss = torch.abs(evalues.view(batch * npoints, 2)).sum(-1)\n",
    "        return eigen_loss.mean()\n",
    "\n",
    "    def forward(self, heatmap, groundtruth):\n",
    "        \"\"\"\n",
    "            heatmap:     b x n x 64 x 64\n",
    "            groundtruth: b x n x 2\n",
    "            output:      b x n x 1 => 1\n",
    "        \"\"\"\n",
    "        # normalize\n",
    "        bs, npoints, h, w = heatmap.shape\n",
    "        heatmap_sum = torch.clamp(heatmap.sum([2, 3]), min=1e-6)\n",
    "        heatmap = heatmap / heatmap_sum.view(bs, npoints, 1, 1)\n",
    "\n",
    "        means = self.weighted_mean(heatmap)  # [bs, 68, 2]\n",
    "        covars = self.unbiased_weighted_covariance(\n",
    "            heatmap, means)  # covars [bs, 68, 2, 2]\n",
    "\n",
    "        # TODO: GPU-based eigen-decomposition\n",
    "        # https://github.com/pytorch/pytorch/issues/60537\n",
    "        _covars = covars.view(bs * npoints, 2, 2).cpu()\n",
    "        # evalues [bs * 68, 2], evectors [bs * 68, 2, 2]\n",
    "        evalues, evectors = _covars.symeig(eigenvectors=True)\n",
    "        evalues = evalues.view(bs, npoints, 2).to(heatmap)\n",
    "        evectors = evectors.view(bs, npoints, 2, 2).to(heatmap)\n",
    "\n",
    "        # STAR Loss\n",
    "        # Ambiguity-guided Decomposition\n",
    "        error = self.ambiguity_guided_decompose(\n",
    "            groundtruth - means, evalues, evectors)\n",
    "        loss_trans = self.dist_func(torch.zeros_like(error).to(error), error)\n",
    "        # Eigenvalue Restriction\n",
    "        loss_eigen = self.eigenvalue_restriction(evalues, bs, npoints)\n",
    "        star_loss = loss_trans + self.w * loss_eigen\n",
    "\n",
    "        return star_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93c9ff1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.266959Z",
     "iopub.status.busy": "2025-05-04T16:17:31.266757Z",
     "iopub.status.idle": "2025-05-04T16:17:31.282055Z",
     "shell.execute_reply": "2025-05-04T16:17:31.281456Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.266944Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [4] After defining loss functions!!!!\n"
     ]
    }
   ],
   "source": [
    "# [LOG]\n",
    "print(\">> [4] After defining loss functions!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44e2baf",
   "metadata": {},
   "source": [
    "# 5. Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d62fde",
   "metadata": {},
   "source": [
    "## 5.1. Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca4ec1dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.282967Z",
     "iopub.status.busy": "2025-05-04T16:17:31.282749Z",
     "iopub.status.idle": "2025-05-04T16:17:31.294250Z",
     "shell.execute_reply": "2025-05-04T16:17:31.293607Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.282948Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "class Accuracy:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Accuracy()\"\n",
    "\n",
    "    def test(self, label_pd, label_gt, ignore_label=-1):\n",
    "        correct_cnt = 0\n",
    "        total_cnt = 0\n",
    "        with torch.no_grad():\n",
    "            label_pd = F.softmax(label_pd, dim=1)\n",
    "            label_pd = torch.max(label_pd, 1)[1]\n",
    "            label_gt = label_gt.long()\n",
    "            c = (label_pd == label_gt)\n",
    "            correct_cnt = torch.sum(c).item()\n",
    "            total_cnt = c.size(0) - torch.sum(label_gt==ignore_label).item()\n",
    "        return correct_cnt, total_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4a44ba",
   "metadata": {},
   "source": [
    "## 5.2. FR_AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "059daaa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.295185Z",
     "iopub.status.busy": "2025-05-04T16:17:31.294978Z",
     "iopub.status.idle": "2025-05-04T16:17:31.309742Z",
     "shell.execute_reply": "2025-05-04T16:17:31.309216Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.295169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# FR_AUC\n",
    "class FR_AUC:\n",
    "    \n",
    "    def __init__(self, data_definition):\n",
    "        self.data_definition = data_definition\n",
    "        if data_definition == '300W':\n",
    "            self.thresh = 0.05\n",
    "        else:\n",
    "            self.thresh = 0.1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"FR_AUC()\"\n",
    "\n",
    "    def test(self, nmes, thres=None, step=0.0001):\n",
    "        if thres is None:\n",
    "            thres = self.thresh\n",
    "\n",
    "        num_data = len(nmes)\n",
    "        xs = np.arange(0, thres + step, step)\n",
    "        ys = np.array([np.count_nonzero(nmes <= x) for x in xs]) / float(num_data)\n",
    "        fr = 1.0 - ys[-1]\n",
    "        auc = simps(ys, x=xs) / thres\n",
    "        return [round(fr, 4), round(auc, 6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6fd525",
   "metadata": {},
   "source": [
    "## 5.3. NME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "208dde84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.310487Z",
     "iopub.status.busy": "2025-05-04T16:17:31.310332Z",
     "iopub.status.idle": "2025-05-04T16:17:31.323239Z",
     "shell.execute_reply": "2025-05-04T16:17:31.322595Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.310475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Normalized Mean Error\n",
    "class NME:\n",
    "    \n",
    "    def __init__(self, nme_left_index, nme_right_index):\n",
    "        self.nme_left_index = nme_left_index\n",
    "        self.nme_right_index = nme_right_index\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"NME()\"\n",
    "\n",
    "    def get_norm_distance(self, landmarks):\n",
    "        assert isinstance(self.nme_right_index, list), 'the nme_right_index is not list.'\n",
    "        assert isinstance(self.nme_left_index, list), 'the nme_left, index is not list.'\n",
    "        right_pupil = landmarks[self.nme_right_index, :].mean(0)\n",
    "        left_pupil = landmarks[self.nme_left_index, :].mean(0)\n",
    "        norm_distance = np.linalg.norm(right_pupil - left_pupil)\n",
    "        return norm_distance\n",
    "\n",
    "    def test(self, label_pd, label_gt):\n",
    "        nme_list = []\n",
    "        label_pd = label_pd.data.cpu().numpy()\n",
    "        label_gt = label_gt.data.cpu().numpy()\n",
    "\n",
    "        for i in range(label_gt.shape[0]):\n",
    "            landmarks_gt = label_gt[i]\n",
    "            landmarks_pv = label_pd[i]\n",
    "            if isinstance(self.nme_right_index, list):\n",
    "                norm_distance = self.get_norm_distance(landmarks_gt)\n",
    "            elif isinstance(self.nme_right_index, int):\n",
    "                norm_distance = np.linalg.norm(landmarks_gt[self.nme_left_index] - landmarks_gt[self.nme_right_index])\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            landmarks_delta = landmarks_pv - landmarks_gt\n",
    "            nme = (np.linalg.norm(landmarks_delta, axis=1) / norm_distance).mean()\n",
    "            nme_list.append(nme)\n",
    "            # sum_nme += nme\n",
    "            # total_cnt += 1\n",
    "        return nme_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62466b8",
   "metadata": {},
   "source": [
    "## 5.4. count_parameters_in_MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59967aa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.324070Z",
     "iopub.status.busy": "2025-05-04T16:17:31.323891Z",
     "iopub.status.idle": "2025-05-04T16:17:31.337544Z",
     "shell.execute_reply": "2025-05-04T16:17:31.337062Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.324056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Count parameters in MB\n",
    "def count_parameters_in_MB(model):\n",
    "    if isinstance(model, nn.Module):\n",
    "        return sum(v.numel() for v in model.parameters()) / 1e6\n",
    "    else:\n",
    "        return sum(v.numel() for v in model) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "faddfd29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.338597Z",
     "iopub.status.busy": "2025-05-04T16:17:31.338356Z",
     "iopub.status.idle": "2025-05-04T16:17:31.349838Z",
     "shell.execute_reply": "2025-05-04T16:17:31.349105Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.338574Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [5] After defining metrics!!!!\n"
     ]
    }
   ],
   "source": [
    "# [LOG]\n",
    "print(\">> [5] After defining metrics!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba0f845",
   "metadata": {},
   "source": [
    "# 6. Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8849e151",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.351320Z",
     "iopub.status.busy": "2025-05-04T16:17:31.350485Z",
     "iopub.status.idle": "2025-05-04T16:17:31.362308Z",
     "shell.execute_reply": "2025-05-04T16:17:31.361720Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.351304Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Face300WXMLDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset cho iBUG 300-W với XML annotations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, xml_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        xml_path = os.path.join(root_dir, xml_file)\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        for img in root.findall('images/image'):\n",
    "            rel_path = img.get('file')\n",
    "            img_path = os.path.join(root_dir, rel_path)\n",
    "            parts = img.find('box').findall('part')\n",
    "            lm = np.zeros((len(parts), 2), dtype=np.float16)\n",
    "            for p in parts:\n",
    "                idx = int(p.get('name'))\n",
    "                lm[idx, 0] = np.float16(p.get('x'))\n",
    "                lm[idx, 1] = np.float16(p.get('y'))\n",
    "            self.samples.append({\"img_path\": img_path, \"landmarks\": lm})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        image = Image.open(sample['img_path']).convert('RGB')\n",
    "        landmarks = torch.from_numpy(sample['landmarks'])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return {\"image\": image, \"landmarks\": landmarks}\n",
    "\n",
    "    # Apply the transform to all samples\n",
    "    def apply_transform(self, transform) -> None:\n",
    "        for sample in self.samples:\n",
    "            image = Image.open(sample['img_path']).convert('RGB')\n",
    "            sample['image'] = transform(image)\n",
    "            sample['landmarks'] = torch.from_numpy(sample['landmarks']).float()\n",
    "\n",
    "    # Set the transform for the dataset\n",
    "    def set_transform(self, transform):\n",
    "        self.transform = transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36683868",
   "metadata": {},
   "source": [
    "## 6.1. Định nghĩa các transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb6ebe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.363164Z",
     "iopub.status.busy": "2025-05-04T16:17:31.362950Z",
     "iopub.status.idle": "2025-05-04T16:17:31.376438Z",
     "shell.execute_reply": "2025-05-04T16:17:31.375923Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.363142Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training transforms với augmentations\n",
    "train_transform = transforms.Compose([\n",
    "    # Resize về 224×224\n",
    "    # ⚙️ độ phân giải đầu vào :contentReference[oaicite:7]{index=7}\n",
    "    transforms.Resize(size=(224,224), interpolation=InterpolationMode.BICUBIC),\n",
    "\n",
    "    # Random affine: rotation ±18°, translate ±5%, scale [0.9,1.1]\n",
    "    transforms.RandomAffine(\n",
    "        # ±18° :contentReference[oaicite:8]{index=8}\n",
    "        degrees=18,\n",
    "        # ±5% kích thước :contentReference[oaicite:9]{index=9}\n",
    "        translate=(0.05, 0.05),\n",
    "        # ±10% scale :contentReference[oaicite:10]{index=10}\n",
    "        scale=(0.9, 1.1)\n",
    "    ),\n",
    "\n",
    "    # Horizontal flip 50%\n",
    "    # :contentReference[oaicite:11]{index=11}\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "    # Grayscale chuyển 20%\n",
    "    # :contentReference[oaicite:12]{index=12}\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "\n",
    "    # Gaussian blur 30%\n",
    "    transforms.RandomApply(\n",
    "        # or (3,3) :contentReference[oaicite:13]{index=13}\n",
    "        [transforms.GaussianBlur(kernel_size=5)],\n",
    "        p=0.3\n",
    "    ),\n",
    "\n",
    "    # Gamma adjustment 20%\n",
    "    transforms.RandomApply(\n",
    "        [transforms.Lambda(lambda img: transforms.functional.adjust_gamma(img,\n",
    "                                                                          gamma=np.random.uniform(0.8, 1.2)))],\n",
    "        p=0.2\n",
    "    ),                                        # :contentReference[oaicite:14]{index=14}\n",
    "\n",
    "    # Occlusion: RandomErasing 40%\n",
    "    transforms.RandomErasing(p=0.4,\n",
    "                             scale=(0.02, 0.25),\n",
    "                             ratio=(0.3, 3.3),\n",
    "                             value='random'),   # :contentReference[oaicite:15]{index=15}\n",
    "\n",
    "    # Cuối cùng: ToTensor + Normalize ImageNet\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b74375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.377423Z",
     "iopub.status.busy": "2025-05-04T16:17:31.377152Z",
     "iopub.status.idle": "2025-05-04T16:17:31.391644Z",
     "shell.execute_reply": "2025-05-04T16:17:31.391094Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.377408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0a66b9",
   "metadata": {},
   "source": [
    "## 6.2. Khởi tạo DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01fbfbca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:31.392529Z",
     "iopub.status.busy": "2025-05-04T16:17:31.392322Z",
     "iopub.status.idle": "2025-05-04T16:17:34.767634Z",
     "shell.execute_reply": "2025-05-04T16:17:34.766871Z",
     "shell.execute_reply.started": "2025-05-04T16:17:31.392507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Đường dẫn dataset\n",
    "if os.path.exists('./ibug_300W_large_face_landmark_dataset'):\n",
    "    root = './ibug_300W_large_face_landmark_dataset'\n",
    "else:\n",
    "    root = '/kaggle/input/ibug-300w-large-face-landmark-dataset/ibug_300W_large_face_landmark_dataset'\n",
    "train_xml = 'labels_ibug_300W_train.xml'\n",
    "test_xml = 'labels_ibug_300W_test.xml'\n",
    "\n",
    "# Dataset\n",
    "full_train_ds = Face300WXMLDataset(root, train_xml, transform=train_transform)\n",
    "test_ds = Face300WXMLDataset(root, test_xml, transform=val_test_transform)\n",
    "\n",
    "# Chia train/val 90/10, seed cố định\n",
    "val_count = int(0.1 * len(full_train_ds))\n",
    "train_count = len(full_train_ds) - val_count\n",
    "generator = torch.Generator().manual_seed(0)\n",
    "train_ds, val_ds = random_split(full_train_ds,\n",
    "                                [train_count, val_count],\n",
    "                                generator=generator)\n",
    "\n",
    "# Set transform cho train/val/test\n",
    "train_ds.dataset.set_transform(train_transform)\n",
    "val_ds.dataset.set_transform(val_test_transform)\n",
    "test_ds.set_transform(val_test_transform)\n",
    "\n",
    "# DataLoader với batch_size=48\n",
    "train_loader = DataLoader(train_ds, batch_size=48, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=48, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=48, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1c68a2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:34.769016Z",
     "iopub.status.busy": "2025-05-04T16:17:34.768712Z",
     "iopub.status.idle": "2025-05-04T16:17:36.234793Z",
     "shell.execute_reply": "2025-05-04T16:17:36.234054Z",
     "shell.execute_reply.started": "2025-05-04T16:17:34.768990Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch: images torch.Size([48, 3, 224, 224]), landmarks torch.Size([48, 68, 2])\n",
      "Train dataset size: 6000\n",
      "Validation dataset size: 666\n",
      "Test dataset size: 1008\n"
     ]
    }
   ],
   "source": [
    "# Iterate\n",
    "for batch in train_loader:\n",
    "    imgs = batch['image']       # (B,3,224,224)\n",
    "    lms = batch['landmarks']    # (B,68,2)\n",
    "    print(f\"Train batch: images {imgs.shape}, landmarks {lms.shape}\")\n",
    "    break\n",
    "\n",
    "# Print number of samples in each dataset\n",
    "print(f\"Train dataset size: {len(train_ds)}\")\n",
    "print(f\"Validation dataset size: {len(val_ds)}\")\n",
    "print(f\"Test dataset size: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01b4d18b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:36.235797Z",
     "iopub.status.busy": "2025-05-04T16:17:36.235529Z",
     "iopub.status.idle": "2025-05-04T16:17:36.239934Z",
     "shell.execute_reply": "2025-05-04T16:17:36.239266Z",
     "shell.execute_reply.started": "2025-05-04T16:17:36.235779Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [6] After defining dataset and dataloader!!!!\n"
     ]
    }
   ],
   "source": [
    "# [LOG]\n",
    "print(\">> [6] After defining dataset and dataloader!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf3af2b",
   "metadata": {},
   "source": [
    "# 7. Training Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289b7caf",
   "metadata": {},
   "source": [
    "## 7.1. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb4bd0d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:36.240991Z",
     "iopub.status.busy": "2025-05-04T16:17:36.240745Z",
     "iopub.status.idle": "2025-05-04T16:17:36.320884Z",
     "shell.execute_reply": "2025-05-04T16:17:36.320101Z",
     "shell.execute_reply.started": "2025-05-04T16:17:36.240967Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 12\n",
    "batch_size = 48\n",
    "initial_lr = 1e-4\n",
    "weight_decay = 1e-5\n",
    "milestones = [6, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31ceba29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:36.322302Z",
     "iopub.status.busy": "2025-05-04T16:17:36.321901Z",
     "iopub.status.idle": "2025-05-04T16:17:36.345487Z",
     "shell.execute_reply": "2025-05-04T16:17:36.344877Z",
     "shell.execute_reply.started": "2025-05-04T16:17:36.322261Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Print the used device\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e59f160",
   "metadata": {},
   "source": [
    "## 7.2. Model, Losses, Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b84adf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:36.346590Z",
     "iopub.status.busy": "2025-05-04T16:17:36.346314Z",
     "iopub.status.idle": "2025-05-04T16:17:39.297449Z",
     "shell.execute_reply": "2025-05-04T16:17:39.296901Z",
     "shell.execute_reply.started": "2025-05-04T16:17:36.346571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Backbone choice: 'mobilenet', 'resnet101', 'convnext_b', 'swin_b'\n",
    "def get_model(backbone_name='swin_b'):\n",
    "    model = FaceXFormer(backbone_name=backbone_name)\n",
    "\n",
    "    # Initialize the parameters\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            # Add seed for reproducibility\n",
    "            nn.init.xavier_uniform_(\n",
    "                p, generator=torch.Generator().manual_seed(0))\n",
    "        else:\n",
    "            nn.init.zeros_(p)\n",
    "\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "# Load the model\n",
    "backbone_name: str = 'swin_b'\n",
    "model = get_model(backbone_name=backbone_name)\n",
    "\n",
    "\n",
    "# Landmark loss\n",
    "landmark_loss_fn = WingLoss().to(device)\n",
    "\n",
    "# Metrics\n",
    "nme_metric = NME(nme_left_index=[36, 39],\n",
    "                 nme_right_index=[42, 45])  # eye landmarks for normalization\n",
    "# uses simps for AUC :contentReference[oaicite:3]{index=3}\n",
    "fr_auc_metric = FR_AUC(data_definition='300W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf9ce410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:39.298281Z",
     "iopub.status.busy": "2025-05-04T16:17:39.298079Z",
     "iopub.status.idle": "2025-05-04T16:17:39.304204Z",
     "shell.execute_reply": "2025-05-04T16:17:39.303444Z",
     "shell.execute_reply.started": "2025-05-04T16:17:39.298264Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA enabled with automatic mixed precision\n",
      "GPU memory allocated: 0.37 GB\n",
      "Model size: 90.96 MB\n"
     ]
    }
   ],
   "source": [
    "# Enable CUDA optimization for half precision (if available)\n",
    "if device.type == 'cuda':\n",
    "    # Enable autocast for mixed precision training\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    print(\"CUDA enabled with automatic mixed precision\")\n",
    "else:\n",
    "    scaler = None\n",
    "    print(\"Running on CPU, mixed precision not enabled\")\n",
    "\n",
    "# Report memory usage\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    print(f\"Model size: {count_parameters_in_MB(model):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e11391",
   "metadata": {},
   "source": [
    "## 7.3. Optimizer and Scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0fe8f931",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:39.305119Z",
     "iopub.status.busy": "2025-05-04T16:17:39.304931Z",
     "iopub.status.idle": "2025-05-04T16:17:39.318715Z",
     "shell.execute_reply": "2025-05-04T16:17:39.318120Z",
     "shell.execute_reply.started": "2025-05-04T16:17:39.305104Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=initial_lr,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "scheduler = MultiStepLR(optimizer, milestones=milestones, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a9a7f4",
   "metadata": {},
   "source": [
    "## 7.4. Training & Validation Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c72caae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:39.319512Z",
     "iopub.status.busy": "2025-05-04T16:17:39.319347Z",
     "iopub.status.idle": "2025-05-04T16:17:39.330202Z",
     "shell.execute_reply": "2025-05-04T16:17:39.329696Z",
     "shell.execute_reply.started": "2025-05-04T16:17:39.319499Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def denorm_points(points, h, w, align_corners=False):\n",
    "    if align_corners:\n",
    "        denorm_points = (points + 1) / 2 * torch.tensor([w - 1, h - 1], dtype=torch.float32).to(points).view(1, 1, 2)\n",
    "    else:\n",
    "        denorm_points = ((points + 1) * torch.tensor([w, h], dtype=torch.float32).to(points).view(1, 1, 2) - 1) / 2\n",
    "\n",
    "    return denorm_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70bdd9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:17:39.331121Z",
     "iopub.status.busy": "2025-05-04T16:17:39.330941Z",
     "iopub.status.idle": "2025-05-04T17:08:07.878329Z",
     "shell.execute_reply": "2025-05-04T17:08:07.877634Z",
     "shell.execute_reply.started": "2025-05-04T16:17:39.331100Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 [Train]: 100%|██████████| 125/125 [04:37<00:00,  2.22s/it, loss=559.0471, lr=0.000100]\n",
      "Epoch 1/12 [Valid]: 100%|██████████| 14/14 [00:22<00:00,  1.63s/it, NME=4.7109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New best model saved! NME: 4.2634\n",
      "Epoch 01 | Train Loss: 726.7489 | Val NME: 4.2634 | Train Time: 277.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 [Train]: 100%|██████████| 125/125 [03:53<00:00,  1.86s/it, loss=650.4006, lr=0.000100]\n",
      "Epoch 2/12 [Valid]: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it, NME=4.6963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New best model saved! NME: 4.2479\n",
      "Epoch 02 | Train Loss: 724.9433 | Val NME: 4.2479 | Train Time: 233.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 [Train]: 100%|██████████| 125/125 [03:48<00:00,  1.83s/it, loss=810.1794, lr=0.000100]\n",
      "Epoch 3/12 [Valid]: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it, NME=4.6820]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New best model saved! NME: 4.2329\n",
      "Epoch 03 | Train Loss: 723.1337 | Val NME: 4.2329 | Train Time: 228.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 [Train]: 100%|██████████| 125/125 [03:48<00:00,  1.83s/it, loss=705.6045, lr=0.000100]\n",
      "Epoch 4/12 [Valid]: 100%|██████████| 14/14 [00:17<00:00,  1.28s/it, NME=4.6668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New best model saved! NME: 4.2168\n",
      "Epoch 04 | Train Loss: 721.3354 | Val NME: 4.2168 | Train Time: 228.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/12 [Train]: 100%|██████████| 125/125 [03:51<00:00,  1.85s/it, loss=721.2102, lr=0.000100]\n",
      "Epoch 5/12 [Valid]: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it, NME=4.6519]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New best model saved! NME: 4.2010\n",
      "Epoch 05 | Train Loss: 719.5477 | Val NME: 4.2010 | Train Time: 231.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/12 [Train]: 100%|██████████| 125/125 [03:49<00:00,  1.83s/it, loss=780.5964, lr=0.000100]\n",
      "Epoch 6/12 [Valid]: 100%|██████████| 14/14 [00:17<00:00,  1.27s/it, NME=4.6375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New best model saved! NME: 4.1854\n",
      "Epoch 06 | Train Loss: 717.7533 | Val NME: 4.1854 | Train Time: 229.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/12 [Train]: 100%|██████████| 125/125 [03:50<00:00,  1.84s/it, loss=773.6861, lr=0.000010]\n",
      "Epoch 7/12 [Valid]: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it, NME=4.6362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New best model saved! NME: 4.1841\n",
      "Epoch 07 | Train Loss: 716.7623 | Val NME: 4.1841 | Train Time: 230.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/12 [Train]: 100%|██████████| 125/125 [03:49<00:00,  1.84s/it, loss=816.4785, lr=0.000010]\n",
      "Epoch 8/12 [Valid]: 100%|██████████| 14/14 [00:17<00:00,  1.25s/it, NME=4.6349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New best model saved! NME: 4.1828\n",
      "Epoch 08 | Train Loss: 716.6216 | Val NME: 4.1828 | Train Time: 229.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/12 [Train]: 100%|██████████| 125/125 [03:49<00:00,  1.84s/it, loss=718.1559, lr=0.000010]\n",
      "Epoch 9/12 [Valid]: 100%|██████████| 14/14 [00:18<00:00,  1.29s/it, NME=4.6335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New best model saved! NME: 4.1812\n",
      "Epoch 09 | Train Loss: 716.4249 | Val NME: 4.1812 | Train Time: 229.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/12 [Train]: 100%|██████████| 125/125 [03:50<00:00,  1.84s/it, loss=711.9120, lr=0.000010]\n",
      "Epoch 10/12 [Valid]: 100%|██████████| 14/14 [00:17<00:00,  1.26s/it, NME=4.6314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New best model saved! NME: 4.1790\n",
      "Epoch 10 | Train Loss: 716.2188 | Val NME: 4.1790 | Train Time: 230.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/12 [Train]: 100%|██████████| 125/125 [03:48<00:00,  1.83s/it, loss=674.1871, lr=0.000001]\n",
      "Epoch 11/12 [Valid]: 100%|██████████| 14/14 [00:17<00:00,  1.25s/it, NME=4.6313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New best model saved! NME: 4.1788\n",
      "Epoch 11 | Train Loss: 716.0880 | Val NME: 4.1788 | Train Time: 228.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/12 [Train]: 100%|██████████| 125/125 [03:49<00:00,  1.83s/it, loss=760.7913, lr=0.000001]\n",
      "Epoch 12/12 [Valid]: 100%|██████████| 14/14 [00:17<00:00,  1.25s/it, NME=4.6312]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New best model saved! NME: 4.1788\n",
      "Epoch 12 | Train Loss: 716.0749 | Val NME: 4.1788 | Train Time: 229.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_nme = float('inf')\n",
    "best_model_wts = None\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\")\n",
    "    for batch in train_pbar:\n",
    "        imgs = batch['image'].to(device)\n",
    "        targets = batch['landmarks'].to(device, dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, dtype=torch.float16):\n",
    "            preds = model(imgs, labels=None, tasks=None)\n",
    "            denorm_lms = denorm_points(preds.view(-1, 68, 2), 224, 224)\n",
    "            loss = landmark_loss_fn(denorm_lms, targets)\n",
    "\n",
    "        # Scales loss. Calls backward() on scaled loss to create scaled gradients.\n",
    "        scaler.scale(loss).backward()\n",
    "        # Unscales gradients and calls optimizer.step() if gradients are not NaN\n",
    "        scaler.step(optimizer)\n",
    "        # Updates the scale for next iteration\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        train_pbar.set_postfix(\n",
    "            {'loss': f\"{loss.item():.4f}\", 'lr': f\"{optimizer.param_groups[0]['lr']:.6f}\"})\n",
    "\n",
    "    train_time = time.time() - epoch_start\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Free memory\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    nmes = []\n",
    "    val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\")\n",
    "    with torch.no_grad():\n",
    "        for batch in val_pbar:\n",
    "            imgs = batch['image'].to(device)\n",
    "            targets = batch['landmarks'].to(device, dtype=torch.float32)\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.float16):\n",
    "                preds = model(imgs, labels=None, tasks=None)\n",
    "                denorm_lms = denorm_points(preds.view(-1, 68, 2), 224, 224)\n",
    "                batch_nmes = nme_metric.test(denorm_lms, targets)\n",
    "\n",
    "            nmes.extend(batch_nmes)\n",
    "            current_nme = sum(batch_nmes) / \\\n",
    "                len(batch_nmes) if batch_nmes else 0\n",
    "            val_pbar.set_postfix({'NME': f\"{current_nme:.4f}\"})\n",
    "\n",
    "    mean_nme = sum(nmes) / len(nmes)\n",
    "\n",
    "    # Free memory\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Scheduler step - this is correct as we're doing it once per epoch\n",
    "    # after all optimizer steps are completed\n",
    "    scheduler.step()\n",
    "\n",
    "    # Check for best\n",
    "    if mean_nme < best_nme:\n",
    "        # Free memory\n",
    "        del best_model_wts\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Save the best model\n",
    "        best_nme = mean_nme\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        print(f\"✅ New best model saved! NME: {mean_nme:.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} \"\n",
    "          f\"| Val NME: {mean_nme:.4f} | Train Time: {train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eddae486",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:08:07.879459Z",
     "iopub.status.busy": "2025-05-04T17:08:07.879242Z",
     "iopub.status.idle": "2025-05-04T17:08:07.883387Z",
     "shell.execute_reply": "2025-05-04T17:08:07.882718Z",
     "shell.execute_reply.started": "2025-05-04T17:08:07.879442Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [7] After training the model!!!!\n"
     ]
    }
   ],
   "source": [
    "# [LOG]\n",
    "print(\">> [7] After training the model!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbd909c",
   "metadata": {},
   "source": [
    "# 8. Final Evaluation on Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec6dde92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:08:07.884384Z",
     "iopub.status.busy": "2025-05-04T17:08:07.884112Z",
     "iopub.status.idle": "2025-05-04T17:08:43.096732Z",
     "shell.execute_reply": "2025-05-04T17:08:43.096139Z",
     "shell.execute_reply.started": "2025-05-04T17:08:07.884360Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Testing]: 100%|██████████| 21/21 [00:35<00:00,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "  Mean NME    : 4.2830\n",
      "  FR (@0.05)  : 1.0000\n",
      "  AUC         : 0.000000\n",
      "  Inference/s : 479.7 img/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(best_model_wts)\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on test\n",
    "test_nmes = []\n",
    "inference_times = []\n",
    "with torch.no_grad():\n",
    "    test_pbar = tqdm(test_loader, desc=\"[Testing]\")\n",
    "    for batch in test_pbar:\n",
    "        imgs = batch['image'].to(device)\n",
    "        targets = batch['landmarks'].to(device, dtype=torch.float32)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, dtype=torch.float16):\n",
    "            start = time.time()\n",
    "            preds = model(imgs, labels=None, tasks=None)\n",
    "            end = time.time()\n",
    "\n",
    "            denorm_lms = denorm_points(preds.view(-1, 68, 2), 224, 224)\n",
    "            test_nmes.extend(nme_metric.test(denorm_lms, targets))\n",
    "\n",
    "        inference_times.append((end - start) / imgs.size(0))\n",
    "\n",
    "# Metrics\n",
    "mean_test_nme = sum(test_nmes) / len(test_nmes)\n",
    "# [FR, AUC] using Simpson's rule :contentReference[oaicite:9]{index=9}\n",
    "fr, auc = fr_auc_metric.test(test_nmes)\n",
    "mean_inference_time = sum(inference_times) / len(inference_times)\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"  Mean NME    : {mean_test_nme:.4f}\")\n",
    "print(f\"  FR (@0.05)  : {fr:.4f}\")\n",
    "print(f\"  AUC         : {auc:.6f}\")\n",
    "print(f\"  Inference/s : {1/mean_inference_time:.1f} img/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe74aec5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:08:43.100975Z",
     "iopub.status.busy": "2025-05-04T17:08:43.100709Z",
     "iopub.status.idle": "2025-05-04T17:08:43.104568Z",
     "shell.execute_reply": "2025-05-04T17:08:43.104000Z",
     "shell.execute_reply.started": "2025-05-04T17:08:43.100959Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [8] After evaluating the model on test set!!!!\n"
     ]
    }
   ],
   "source": [
    "# [LOG]\n",
    "print(\">> [8] After evaluating the model on test set!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808a5fbf",
   "metadata": {},
   "source": [
    "# 9. Save best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66e3992b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:08:43.105461Z",
     "iopub.status.busy": "2025-05-04T17:08:43.105195Z",
     "iopub.status.idle": "2025-05-04T17:08:43.951353Z",
     "shell.execute_reply": "2025-05-04T17:08:43.950472Z",
     "shell.execute_reply.started": "2025-05-04T17:08:43.105440Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to ./models/facexformer_swin_b_best.pth\n",
      "Model metrics - NME: 4.2830, FR@0.05: 1.0000, AUC: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Create directory for saving models if it doesn't exist\n",
    "save_dir = './models'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save the best model weights\n",
    "model_save_path = os.path.join(\n",
    "    save_dir, f'facexformer_{backbone_name}_best.pth')\n",
    "torch.save({\n",
    "    # Save backbone name\n",
    "    'backbone_name': backbone_name,\n",
    "    # Save model state dict\n",
    "    'state_dict': best_model_wts,\n",
    "    # Save metric values on test set\n",
    "    'metrics': {\n",
    "        'nme': mean_test_nme,\n",
    "        'fr': fr,\n",
    "        'auc': auc,\n",
    "        'inference_time': mean_inference_time\n",
    "    },\n",
    "}, model_save_path)\n",
    "\n",
    "print(f\"Best model saved to {model_save_path}\")\n",
    "print(\n",
    "    f\"Model metrics - NME: {mean_test_nme:.4f}, FR@0.05: {fr:.4f}, AUC: {auc:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79470d4",
   "metadata": {},
   "source": [
    "# 10. Load best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc64a71f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:08:43.953982Z",
     "iopub.status.busy": "2025-05-04T17:08:43.952116Z",
     "iopub.status.idle": "2025-05-04T17:08:46.340363Z",
     "shell.execute_reply": "2025-05-04T17:08:46.339718Z",
     "shell.execute_reply.started": "2025-05-04T17:08:43.953961Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FaceXFormer model with swin_b backbone\n",
      "Metrics on test set:\n",
      "  Mean NME    : 4.2830\n",
      "  FR (@0.05)  : 1.0000\n",
      "  AUC         : 0.000000\n",
      "  Inference/s : 479.7 img/s\n"
     ]
    }
   ],
   "source": [
    "# Load the best model with exact backbone\n",
    "def load_facexformer_model(model_path):\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Model file not found at {model_path}\")\n",
    "        return None, None\n",
    "\n",
    "    # Load model checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=device,\n",
    "                            weights_only=False)\n",
    "    backbone_name = checkpoint['backbone_name']\n",
    "\n",
    "    # Initialize model with the same backbone\n",
    "    model = FaceXFormer(backbone_name=backbone_name)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Get metrics\n",
    "    metrics = checkpoint['metrics']\n",
    "\n",
    "    print(f\"Loaded FaceXFormer model with {backbone_name} backbone\")\n",
    "    print(f\"Metrics on test set:\")\n",
    "    print(f\"  Mean NME    : {metrics['nme']:.4f}\")\n",
    "    print(f\"  FR (@0.05)  : {metrics['fr']:.4f}\")\n",
    "    print(f\"  AUC         : {metrics['auc']:.6f}\")\n",
    "    print(f\"  Inference/s : {1/metrics['inference_time']:.1f} img/s\")\n",
    "\n",
    "    return model, metrics\n",
    "\n",
    "\n",
    "# Load the best model\n",
    "model_path = os.path.join(save_dir, f'facexformer_{backbone_name}_best.pth')\n",
    "loaded_model, loaded_metrics = load_facexformer_model(model_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2029045,
     "sourceId": 3364057,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
